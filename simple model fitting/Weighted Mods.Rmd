---
title: "Weighted Mods"
author: "Jason Turk"
date: "2025-04-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Predicting Duration from Learning Commons Data

## MARS, Random Forests, and Boosting

``` {r, echo = FALSE}
# --- Load Libraries ---
# Using suppressPackageStartupMessages to minimize console noise
suppressPackageStartupMessages(library(here)) # For locating files relative to project root
suppressPackageStartupMessages(library(dplyr)) # For data manipulation
suppressPackageStartupMessages(library(rsample)) # For data splitting and CV folds
suppressPackageStartupMessages(library(tune)) # For hyperparameter tuning
suppressPackageStartupMessages(library(yardstick)) # For model evaluation metrics
suppressPackageStartupMessages(library(workflows)) # For combining recipes and models
suppressPackageStartupMessages(library(parsnip)) # For model specifications
suppressPackageStartupMessages(library(recipes)) # For preprocessing
suppressPackageStartupMessages(library(readr)) # For reading/writing data
suppressPackageStartupMessages(library(glue)) # For formatted string output

# --- Source Helper Functions ---
# Construct full paths using here() for robustness
source(here::here("src/r/utils/data_utils.R"))
source(here::here("src/r/recipes/recipes.R"))
source(here::here("src/r/models/models.R"))
source(here::here("src/r/workflows/workflows.R"))
source(here::here("src/r/tuning/tuning.R"))
source(here::here("src/r/training/training.R"))
source(here::here("src/r/evaluation/evaluation.R"))

# --- Configuration ---
# TODO: Move these to a config file/package (e.g., config::get())
DATA_FILENAME <- "train_engineered.csv" # Use only the training data
TEST_SPLIT_PROP <- 0.8 # Proportion of data for training set
TARGET_VARIABLE <- "Duration_In_Min" # Or "Occupancy"
CV_FOLDS <- 5 # Number of cross-validation folds
SEED <- 3 # For reproducibility
TUNING_METRIC <- "rmse" # Metric to select best hyperparameters (use "accuracy" or "roc_auc" for classification)
# Features to drop (matching Python preprocess.py, excluding targets and date columns handled in recipe)
FEATURES_TO_DROP <- c(
    "Student_IDs", "Semester", "Class_Standing", "Major", "Expected_Graduation",
    "Course_Name", "Course_Number", "Course_Type", "Course_Code_by_Thousands",
    "Check_Out_Time", "Session_Length_Category"
) # Add others as needed

# Define metric set based on task (regression in this case)
# TODO: Handle classification metrics if TARGET_VARIABLE changes
MODEL_METRICS <- metric_set(rmse, rsq, mae)


# --- Pipeline Execution ---

set.seed(SEED)

# 1. Load Data
cat(glue::glue("--- Loading Data ({DATA_FILENAME}) ---\n\n"))
full_data <- load_data(DATA_FILENAME)
cat(glue::glue("Full data rows: {nrow(full_data)}\n\n"))

# 2. Split Data into Training and Testing
cat(glue::glue("--- Splitting Data (Train: {TEST_SPLIT_PROP*100}%, Test: {(1-TEST_SPLIT_PROP)*100}%) ---\n\n"))
data_split <- rsample::initial_split(full_data, prop = TEST_SPLIT_PROP, strata = NULL) # Add strata if needed
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)
cat(glue::glue("Training data rows: {nrow(train_data)}, Test data rows: {nrow(test_data)}\n\n"))

# 3. Create Recipe
cat(glue::glue("--- Creating Recipe for Target: {TARGET_VARIABLE} ---\n\n"))
# NOTE: FEATURES_TO_DROP list is now correctly defined above
recipe_obj <- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)
#print(recipe_obj) # Print summary of recipe steps

prepped_recipe <- prep(recipe_obj, training = full_data)
maintrain <- bake(prepped_recipe, new_data = full_data)
```

**For data pre-processing, we simply utilize the same data set constructed last semester but now with outside weather and moon phase data appended. Some of the modified predictors that our training data set contains include an indicator of multiple majors, an overarching major categorical variable, an overarching course category variable, the week of the semester during which a visit happens, the number of months until a student graduates, and so on. The result is a larger feature space (132 predictors in total), but we hope that our models will inherently filter out the noise and select for important predictors naturally, instead of using a separate mechanism for feature selection.**

**Here, we report overall model performance using RMSE on a 20% holdout of the training data after obtaining tuning parameters by using cross-validation within the 'caret' package. Our final models will be selected using a larger grid search which is not shown here.**

``` {r}
testsize <- floor(0.2*nrow(maintrain)) #maintrain is the full training set
set.seed(7560)
hIND <- sample(1:nrow(maintrain), size = testsize)
students <- maintrain[-hIND,]
holdout <- maintrain[hIND,]

studentsfull <- read.csv(here::here("data/processed/train_engineered.csv"))
trainfull <- studentsfull[-hIND,]
holdoutfull <- studentsfull[hIND,]
```

``` {r}
#MARS
library(caret)

set.seed(7560)
tc1 <- trainControl(method = "cv", number = 5, search = "grid")
full1 <- train(Duration_In_Min ~ ., data = students, method = "earth",
               trControl = tc1, tuneLength = 5)
hfit1 <- predict(full1, newdata = holdout)
toterror1 <- sum((hfit1 - holdout$Duration_In_Min)^2)/length(hfit1)
sqrt(toterror1)
```

**The linear models tried last semester achieved RMSE values of around 60, so MARS does not appear substantially better, yielding an RMSE of 59.23. The inclusion of weather data into the training set may also have contributed to the slightly improved model performance. Here we tried a 'tuneLength' of 5 in the caret package for simplicity.**


``` {r}
w1 <- 2/3
w2 <- 1-w1

ID1 <- data.frame(ids = holdoutfull$Student_IDs, hfit1)

ID1$OtherVisits <- c()
for (i in 1:nrow(ID1)) {
  mask <- trainfull$Student_IDs == ID1$ids[i]
  ID1$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID1$Weighted <- ifelse(is.nan(ID1$OtherVisits), ID1$y,
                      w1*ID1$y + w2*ID1$OtherVisits)

toterrorW1 <- sum((ID1$Weighted - holdout$Duration_In_Min)^2)/length(ID1$Weighted)
sqrt(toterrorW1)
```

**We believe that students may repeatedly visit the Learning Commons and stay roughly the same amount of time each visit as it becomes part of their routine habits. To reflect this, we consider a weighting of the model by the mean of their visit durations seen by the training data. Here we employ a 2/3 weight towards the predicted values and a 1/3 weight towards the training visit duration means. If there are no such training visits for a given person, we just return their predicted value. Doing so yields a slight improvement in RMSE, reducing it to 57.79. Using mean visit duration as a predictor is unfeasible since it must be computed from several data points and thus would contaminate any cross-validation procedures we run. Also, weighting by mean visit duration now highlights the relevance of student behaviors in modeling their visit duration. The choice of 1/3 as a weight towards mean visit duration is for now arbitrary; once the final model is selected, the weights will be chosen by cross-validation over the entire training data set.**

``` {r, warning = FALSE}
#Random forests
library(doParallel)

num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
tc2 <- trainControl(method = "cv", number = 5, search = "grid")
full2 <- train(Duration_In_Min ~ ., data = students, method = "rf",
               trControl = tc2, tuneLength = 3)
hfit2 <- predict(full2, newdata = holdout)
toterror2 <- sum((hfit2 - holdout$Duration_In_Min)^2)/length(hfit2)
sqrt(toterror2)

stopCluster(cl)
```

**Random forests performs better, with an RMSE of 57.04, besting the linear models used last semester more clearly. A 'tuneLength' of 3 was used here to speed up computations.**

``` {r}
w1 <- 2/3
w2 <- 1-w1

ID2 <- data.frame(ids = holdoutfull$Student_IDs, Duration_In_Min = hfit2)

ID2$OtherVisits <- c()
for (i in 1:nrow(ID2)) {
  mask <- trainfull$Student_IDs == ID2$ids[i]
  ID2$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID2$Weighted <- ifelse(is.nan(ID2$OtherVisits), ID2$Duration_In_Min,
                      w1*ID2$Duration_In_Min + w2*ID2$OtherVisits)

toterrorW2 <- sum((ID2$Weighted - holdout$Duration_In_Min)^2)/length(ID2$Weighted)
sqrt(toterrorW2)
```

**Using a weighted average of our predicted values and the training mean visit durations is also somewhat useful for random forests, reducing the RMSE to 56.57. Another weight may perform better, but for now we simply observe that this modification of the predicted values improves upon both MARS and random forests.**

``` {r}
#Boosting
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

tc3 <- trainControl(method = "cv", number = 5, search = "grid")
full3 <- train(Duration_In_Min ~ ., data = students, method = "xgbTree",
               trControl = tc3, tuneLength = 3, verbose = FALSE)
hfit3 <- predict(full3, newdata = holdout)
toterror3 <- sum((hfit3 - holdout$Duration_In_Min)^2)/length(hfit3)
sqrt(toterror3)

stopCluster(cl)
```

**Boosting (specifically XGBoost) is slightly worse than random forests, with an RMSE of 58.69. Again a 'tuneLength' of 3 was used.**

``` {r}
w1 <- 2/3
w2 <- 1-w1

ID3 <- data.frame(ids = holdoutfull$Student_IDs, Duration_In_Min = hfit3)

ID3$OtherVisits <- c()
for (i in 1:nrow(ID3)) {
  mask <- trainfull$Student_IDs == ID3$ids[i]
  ID3$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID3$Weighted <- ifelse(is.nan(ID3$OtherVisits), ID3$Duration_In_Min,
                      w1*ID3$Duration_In_Min + w2*ID3$OtherVisits)

toterrorW3 <- sum((ID3$Weighted - holdout$Duration_In_Min)^2)/length(ID3$Weighted)
sqrt(toterrorW3)
```

**Weighting by student mean visit durations also helps the boosting model.**

**Random forests performs the best of our attempted models, and so we choose it to model Duration_In_Min. We now collect the training data together and cross-validate to select an optimal weight for the model. The final tuning parameter values were mtry = 20 (20 predictors used when considering a split), trees = 200, and min_n = 10 (splits only occur at nodes that have at least 10 observations).**

``` {r, warning = FALSE}
# To select w1/w2 via final CV

studentsfull <- read.csv(here::here("data/processed/train_engineered.csv"))
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
testInd <- createFolds(maintrain$Duration_In_Min, k = 5)

IDLIST <- list()
CVMSE <- c()

tune <- data.frame(mtry = 20)
for (i in 1:5) {
  fullF <- train(Duration_In_Min ~ ., data = maintrain[-testInd[[i]],], method = "rf",
                 tuneGrid = tune, ntree = 200, nodesize = 10)

  fittedF <- predict(fullF, newdata = maintrain[testInd[[i]],])
  IDLIST[[i]] <- data.frame(Fold = i,
                            ids = studentsfull$Student_IDs[testInd[[i]]],
                            fittedF,
                            TRUEy = studentsfull$Duration_In_Min[testInd[[i]]])
}

stopCluster(cl)
```

``` {r}
ID <- do.call(rbind, IDLIST)
colnames(ID) <- c("Fold", "ids", "fittedF", "TRUEy")

ID$AveOtherVisits <- NA
for (i in 1:nrow(ID)) {
  current_id <- ID$ids[i]
  current_fold <- ID$Fold[i]
  
  #Return predictions for the same ID in other folds
  mask <- ID$ids == current_id & ID$Fold != current_fold
  ID$AveOtherVisits[i] <- mean(ID$TRUEy[mask])
}


weights <- c(0, 1/6, 1/4, 1/3, 1/2, 2/3, 3/4, 5/6, 1)
#Appears that somewhere in the 1/2 - 3/4 range is good
RMSE <- c()
for (j in 1:length(weights)) {
  w1 <- weights[j]
  w2 <- 1-w1
  ID$Weighted <- ifelse(is.nan(ID$AveOtherVisits), ID$fittedF,
                        w1*ID$fittedF + w2*ID$AveOtherVisits)
  CVMSEW <- c()
  for (i in 1:5) {
    CVMSEW[i] <- sum((ID$Weighted[testInd[[i]]] -
                     ID$TRUEy[testInd[[i]]])^2)/length(testInd[[i]])
  }
  RMSE[j] <- sqrt(sum(CVMSEW)/5)
}

data.frame(weights, RMSE)
```

**Trying a grid of weights, we see that a weight of 3/4 towards our predicted values and a weight of 1/4 towards the mean of a student's training data visit durations is best. Our final predictions for Duration are created below.**

``` {r}
modpreds <- read.csv(here::here("data/predictions/Duration_In_Min_RandomForest_20250423180825_pred.csv"))
fulltest <- read.csv(here::here("data/processed/test_engineered.csv"))

predIDs <- data.frame(modpreds, ids = fulltest$Student_IDs)

predIDs$adj <- NA
for (i in 1:nrow(predIDs)) {
  mask <- ID$ids == predIDs$ids[i]
  predIDs$adj[i] <- mean(ID$TRUEy[mask])
}

w1 <- 3/4
w2 <- 1/4
predIDs$FINAL <- ifelse(is.nan(predIDs$adj), predIDs$.pred,
                           w1*predIDs$.pred + w2*predIDs$adj)
write.csv(predIDs$FINAL, "../data/predictions/Duration_FINAL", row.names = F)
```



# Part 2: Predicting Occupancy from Learning Commons Data

## MARS, Random Forests, and Boosting

**It is worth noting that Occupancy is not expected to be a function of individual student behavior in the same way Duration is, so we do not employ post-prediction weighting of any kind here.**

``` {r, echo = FALSE}
# --- Load Libraries ---
# Using suppressPackageStartupMessages to minimize console noise
suppressPackageStartupMessages(library(here)) # For locating files relative to project root
suppressPackageStartupMessages(library(dplyr)) # For data manipulation
suppressPackageStartupMessages(library(rsample)) # For data splitting and CV folds
suppressPackageStartupMessages(library(tune)) # For hyperparameter tuning
suppressPackageStartupMessages(library(yardstick)) # For model evaluation metrics
suppressPackageStartupMessages(library(workflows)) # For combining recipes and models
suppressPackageStartupMessages(library(parsnip)) # For model specifications
suppressPackageStartupMessages(library(recipes)) # For preprocessing
suppressPackageStartupMessages(library(readr)) # For reading/writing data
suppressPackageStartupMessages(library(glue)) # For formatted string output

# --- Source Helper Functions ---
# Construct full paths using here() for robustness
source(here::here("src/r/utils/data_utils.R"))
source(here::here("src/r/recipes/recipes.R"))
source(here::here("src/r/models/models.R"))
source(here::here("src/r/workflows/workflows.R"))
source(here::here("src/r/tuning/tuning.R"))
source(here::here("src/r/training/training.R"))
source(here::here("src/r/evaluation/evaluation.R"))

# --- Configuration ---
# TODO: Move these to a config file/package (e.g., config::get())
DATA_FILENAME <- "train_engineered.csv" # Use only the training data
TEST_SPLIT_PROP <- 0.8 # Proportion of data for training set
TARGET_VARIABLE <- "Occupancy"
CV_FOLDS <- 5 # Number of cross-validation folds
SEED <- 3 # For reproducibility
TUNING_METRIC <- "rmse" # Metric to select best hyperparameters (use "accuracy" or "roc_auc" for classification)
# Features to drop (matching Python preprocess.py, excluding targets and date columns handled in recipe)
FEATURES_TO_DROP <- c(
    "Student_IDs", "Semester", "Class_Standing", "Major", "Expected_Graduation",
    "Course_Name", "Course_Number", "Course_Type", "Course_Code_by_Thousands",
    "Check_Out_Time", "Session_Length_Category"
) # Add others as needed

# Define metric set based on task (regression in this case)
# TODO: Handle classification metrics if TARGET_VARIABLE changes
MODEL_METRICS <- metric_set(rmse, rsq, mae)


# --- Pipeline Execution ---

set.seed(SEED)

# 1. Load Data
cat(glue::glue("--- Loading Data ({DATA_FILENAME}) ---\n\n"))
full_data <- load_data(DATA_FILENAME)
cat(glue::glue("Full data rows: {nrow(full_data)}\n\n"))

# 2. Split Data into Training and Testing
cat(glue::glue("--- Splitting Data (Train: {TEST_SPLIT_PROP*100}%, Test: {(1-TEST_SPLIT_PROP)*100}%) ---\n\n"))
data_split <- rsample::initial_split(full_data, prop = TEST_SPLIT_PROP, strata = NULL) # Add strata if needed
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)
cat(glue::glue("Training data rows: {nrow(train_data)}, Test data rows: {nrow(test_data)}\n\n"))

# 3. Create Recipe
cat(glue::glue("--- Creating Recipe for Target: {TARGET_VARIABLE} ---\n\n"))
# NOTE: FEATURES_TO_DROP list is now correctly defined above
recipe_obj <- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)
#print(recipe_obj) # Print summary of recipe steps

prepped_recipe <- prep(recipe_obj, training = full_data)
maintrainOCC <- bake(prepped_recipe, new_data = full_data)
```

``` {r, echo = FALSE}
testsizeO <- floor(0.2*nrow(maintrainOCC))
set.seed(7560)
hINDO <- sample(1:nrow(maintrainOCC), size = testsizeO)
studentsO <- maintrainOCC[-hINDO,]
holdoutO <- maintrainOCC[hINDO,]

studentsfullO <- read.csv(here::here("data/processed/train_engineered.csv"))
trainfullO <- studentsfullO[-hINDO,]
holdoutfullO <- studentsfullO[hINDO,]
```

``` {r}
#Cleanup
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

unregister_dopar()
```

``` {r}
#MARS

set.seed(7560)
tc1O <- trainControl(method = "cv", number = 5, search = "grid")
full1O <- train(Occupancy ~ ., data = studentsO, method = "earth",
               trControl = tc1O, tuneLength = 5)
hfit1O <- predict(full1O, newdata = holdoutO)
toterror1O <- sum((hfit1O - holdoutO$Occupancy)^2)/length(hfit1O)
sqrt(toterror1O)
```

**MARS (with a basic 'tuneLength' grid search for hyperparameters) yields an RMSE of 3.75, performing slightly worse than the penalized spline model we selected last semester, which had an RMSE of 3.64.**

``` {r, warning = FALSE}
#Random forests
library(doParallel)

num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
tc2O <- trainControl(method = "cv", number = 5, search = "grid")
full2O <- train(Occupancy ~ ., data = studentsO, method = "rf",
               trControl = tc2O, tuneLength = 3)
hfit2O <- predict(full2O, newdata = holdoutO)
toterror2O <- sum((hfit2O - holdoutO$Occupancy)^2)/length(hfit2O)
sqrt(toterror2O)

stopCluster(cl)
```

**Random forests yields markedly better performance, with an RMSE of 1.83.**

``` {r}
#Boosting
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

tc3O <- trainControl(method = "cv", number = 5, search = "grid")
full3O <- train(Occupancy ~ ., data = studentsO, method = "xgbTree",
               trControl = tc3O, tuneLength = 3, verbose = FALSE)
hfit3O <- predict(full3O, newdata = holdoutO)
toterror3O <- sum((hfit3O - holdoutO$Occupancy)^2)/length(hfit3O)
sqrt(toterror3O)

stopCluster(cl)
```

**XGBoost performs better than MARS and better than the best models we tried last semester (its RMSE is about 2.52), but the clear winner is random forests. The hyperparameters chosen via cross-validation that we settled on for our random forest model were mtry = 20, trees = 200, and min_n = 2, a slightly more flexible model than that used for duration (since min_n is smaller, allowing splits to occur even in nodes with only 2 observations).**

**In both the duration and occupancy problems, we have seen that random forests, while computationally costly, yields substantial improvements on predictive power both compared to the other non-linear models tried here and compared to the linear ones tried last semester. Predictions for both visit duration and occupancy benefit from allowing for non-linear effects in our model.**

``` {r}
modpredsO <- read.csv(here::here("data/predictions/Occupancy_RandomForest_20250423155038_pred.csv"))
colnames(modpredsO) <- "x"
write.csv(modpredsO, "../data/predictions/Occupancy_FINAL", row.names = F)
```
