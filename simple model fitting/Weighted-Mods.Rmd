---
title: "Final Project Report - MATH7560 Sp25"
author: "Emma Liang, Ryan Renken, Jaryt Salvo, Jason Turk"
date: "2025-04-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part 1: Predicting Duration from Learning Commons Data

## MARS, Random Forests, and Boosting

``` {r, echo = FALSE}
# --- Load Libraries ---
# Using suppressPackageStartupMessages to minimize console noise
suppressPackageStartupMessages(library(here)) # For locating files relative to project root
suppressPackageStartupMessages(library(dplyr)) # For data manipulation
suppressPackageStartupMessages(library(rsample)) # For data splitting and CV folds
suppressPackageStartupMessages(library(tune)) # For hyperparameter tuning
suppressPackageStartupMessages(library(yardstick)) # For model evaluation metrics
suppressPackageStartupMessages(library(workflows)) # For combining recipes and models
suppressPackageStartupMessages(library(parsnip)) # For model specifications
suppressPackageStartupMessages(library(recipes)) # For preprocessing
suppressPackageStartupMessages(library(readr)) # For reading/writing data
suppressPackageStartupMessages(library(glue)) # For formatted string output

# --- Source Helper Functions ---
# Construct full paths using here() for robustness
source(here::here("src/r/utils/data_utils.R"))
source(here::here("src/r/recipes/recipes.R"))
source(here::here("src/r/models/models.R"))
source(here::here("src/r/workflows/workflows.R"))
source(here::here("src/r/tuning/tuning.R"))
source(here::here("src/r/training/training.R"))
source(here::here("src/r/evaluation/evaluation.R"))

# --- Configuration ---
# TODO: Move these to a config file/package (e.g., config::get())
DATA_FILENAME <- "train_engineered.csv" # Use only the training data
TEST_SPLIT_PROP <- 0.8 # Proportion of data for training set
TARGET_VARIABLE <- "Duration_In_Min" # Or "Occupancy"
CV_FOLDS <- 5 # Number of cross-validation folds
SEED <- 3 # For reproducibility
TUNING_METRIC <- "rmse" # Metric to select best hyperparameters (use "accuracy" or "roc_auc" for classification)
# Features to drop (matching Python preprocess.py, excluding targets and date columns handled in recipe)
FEATURES_TO_DROP <- c(
    "Student_IDs", "Semester", "Class_Standing", "Major", "Expected_Graduation",
    "Course_Name", "Course_Number", "Course_Type", "Course_Code_by_Thousands",
    "Check_Out_Time", "Session_Length_Category"
) # Add others as needed

# Define metric set based on task (regression in this case)
# TODO: Handle classification metrics if TARGET_VARIABLE changes
MODEL_METRICS <- metric_set(rmse, rsq, mae)


# --- Pipeline Execution ---

set.seed(SEED)

# 1. Load Data
cat(glue::glue("--- Loading Data ({DATA_FILENAME}) ---\n\n"))
full_data <- load_data(DATA_FILENAME)
cat(glue::glue("Full data rows: {nrow(full_data)}\n\n"))

# 2. Split Data into Training and Testing
cat(glue::glue("--- Splitting Data (Train: {TEST_SPLIT_PROP*100}%, Test: {(1-TEST_SPLIT_PROP)*100}%) ---\n\n"))
data_split <- rsample::initial_split(full_data, prop = TEST_SPLIT_PROP, strata = NULL) # Add strata if needed
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)
cat(glue::glue("Training data rows: {nrow(train_data)}, Test data rows: {nrow(test_data)}\n\n"))

# 3. Create Recipe
cat(glue::glue("--- Creating Recipe for Target: {TARGET_VARIABLE} ---\n\n"))
# NOTE: FEATURES_TO_DROP list is now correctly defined above
recipe_obj <- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)
#print(recipe_obj) # Print summary of recipe steps

prepped_recipe <- prep(recipe_obj, training = full_data)
maintrain <- bake(prepped_recipe, new_data = full_data)
```

**For data pre-processing, we simply utilize the same data set constructed last semester but now with outside weather and moon phase data appended. Some of the modified predictors that our training data set contains include an indicator of multiple majors, an overarching major categorical variable, an overarching course category variable, the week of the semester during which a visit happens, the number of months until a student graduates, and so on. The result is a larger feature space (132 predictors in total), but we hope that our models will inherently filter out the noise and select for important predictors naturally, instead of using a separate mechanism for feature selection.**

**Here, we report overall model performance using RMSE on a 20% holdout of the training data after obtaining tuning parameters by using cross-validation within the 'caret' package. Our final models will be selected using a larger grid search which is not shown here.**

``` {r}
testsize <- floor(0.2*nrow(maintrain)) #maintrain is the full training set
set.seed(7560)
hIND <- sample(1:nrow(maintrain), size = testsize)
students <- maintrain[-hIND,]
holdout <- maintrain[hIND,]

studentsfull <- read.csv(here::here("data/processed/train_engineered.csv"))
trainfull <- studentsfull[-hIND,]
holdoutfull <- studentsfull[hIND,]
```

``` {r}
#MARS
library(caret)

set.seed(7560)
tc1 <- trainControl(method = "cv", number = 5, search = "grid")
full1 <- train(Duration_In_Min ~ ., data = students, method = "earth",
               trControl = tc1, tuneLength = 5)
hfit1 <- predict(full1, newdata = holdout)
toterror1 <- sum((hfit1 - holdout$Duration_In_Min)^2)/length(hfit1)
sqrt(toterror1)
```

**The linear models tried last semester achieved RMSE values of around 60, so MARS does not appear substantially better, yielding an RMSE of 59.23. The inclusion of weather data into the training set may also have contributed to the slightly improved model performance. Here we tried a 'tuneLength' of 5 in the caret package for simplicity.**


``` {r}
w1 <- 2/3
w2 <- 1-w1

ID1 <- data.frame(ids = holdoutfull$Student_IDs, hfit1)

ID1$OtherVisits <- c()
for (i in 1:nrow(ID1)) {
  mask <- trainfull$Student_IDs == ID1$ids[i]
  ID1$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID1$Weighted <- ifelse(is.nan(ID1$OtherVisits), ID1$y,
                      w1*ID1$y + w2*ID1$OtherVisits)

toterrorW1 <- sum((ID1$Weighted - holdout$Duration_In_Min)^2)/length(ID1$Weighted)
sqrt(toterrorW1)
```

**We believe that students may repeatedly visit the Learning Commons and stay roughly the same amount of time each visit as it becomes part of their routine habits. To reflect this, we consider a weighting of the model by the mean of their visit durations seen by the training data. Here we employ a 2/3 weight towards the predicted values and a 1/3 weight towards the training visit duration means. If there are no such training visits for a given person, we just return their predicted value. Doing so yields a slight improvement in RMSE, reducing it to 57.79. Using mean visit duration as a predictor is unfeasible since it must be computed from several data points and thus would contaminate any cross-validation procedures we run. Also, weighting by mean visit duration now highlights the relevance of student behaviors in modeling their visit duration. The choice of 1/3 as a weight towards mean visit duration is for now arbitrary; once the final model is selected, the weights will be chosen by cross-validation over the entire training data set.**

``` {r, warning = FALSE}
#Random forests
library(doParallel)

num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
tc2 <- trainControl(method = "cv", number = 5, search = "grid")
full2 <- train(Duration_In_Min ~ ., data = students, method = "rf",
               trControl = tc2, tuneLength = 3)
hfit2 <- predict(full2, newdata = holdout)
toterror2 <- sum((hfit2 - holdout$Duration_In_Min)^2)/length(hfit2)
sqrt(toterror2)

stopCluster(cl)
```

**Random forests performs better, with an RMSE of 57.04, besting the linear models used last semester more clearly. A 'tuneLength' of 3 was used here to speed up computations.**

``` {r}
w1 <- 2/3
w2 <- 1-w1

ID2 <- data.frame(ids = holdoutfull$Student_IDs, Duration_In_Min = hfit2)

ID2$OtherVisits <- c()
for (i in 1:nrow(ID2)) {
  mask <- trainfull$Student_IDs == ID2$ids[i]
  ID2$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID2$Weighted <- ifelse(is.nan(ID2$OtherVisits), ID2$Duration_In_Min,
                      w1*ID2$Duration_In_Min + w2*ID2$OtherVisits)

toterrorW2 <- sum((ID2$Weighted - holdout$Duration_In_Min)^2)/length(ID2$Weighted)
sqrt(toterrorW2)
```

**Using a weighted average of our predicted values and the training mean visit durations is also somewhat useful for random forests, reducing the RMSE to 56.57. Another weight may perform better, but for now we simply observe that this modification of the predicted values improves upon both MARS and random forests.**

``` {r}
#Boosting
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

tc3 <- trainControl(method = "cv", number = 5, search = "grid")
full3 <- train(Duration_In_Min ~ ., data = students, method = "xgbTree",
               trControl = tc3, tuneLength = 3, verbose = FALSE)
hfit3 <- predict(full3, newdata = holdout)
toterror3 <- sum((hfit3 - holdout$Duration_In_Min)^2)/length(hfit3)
sqrt(toterror3)

stopCluster(cl)
```

**Boosting (specifically XGBoost) is slightly worse than random forests, with an RMSE of 58.69. Again a 'tuneLength' of 3 was used.**

``` {r}
w1 <- 2/3
w2 <- 1-w1

ID3 <- data.frame(ids = holdoutfull$Student_IDs, Duration_In_Min = hfit3)

ID3$OtherVisits <- c()
for (i in 1:nrow(ID3)) {
  mask <- trainfull$Student_IDs == ID3$ids[i]
  ID3$OtherVisits[i] <- mean(trainfull$Duration_In_Min[mask])
}

ID3$Weighted <- ifelse(is.nan(ID3$OtherVisits), ID3$Duration_In_Min,
                      w1*ID3$Duration_In_Min + w2*ID3$OtherVisits)

toterrorW3 <- sum((ID3$Weighted - holdout$Duration_In_Min)^2)/length(ID3$Weighted)
sqrt(toterrorW3)
```

**Weighting by student mean visit durations also helps the boosting model.**

## Neural Networks

``` {r}
library(reticulate)
```

**In addition to the XGBoost, MARS, and Random Forest models just discussed, we also wanted to explore how neural networks would perform on our two tasks.**

**In this section, we will construct a simple feedforward neural network in Python using the PyTorch library. As there are endless possible combinations of architectures paired with different hyperparameters, we will use 5 Fold cross validation to train/validate a subset of possible models. We also sampled a set of holdout test data to evaluate the overall performances of each model on data not seen during training.**

**To avoid copying an overwhelming amount of python code in our report, we've only included simplified code chunks to showcase our model training and evaluation processes.**

***Note: In this section, we will not incorporate the post-hoc visit weighting scheme we implemented for the previous models; though it could be done, we will see that random forests is the best model and elect to perform the weighting for that model instead.***

### Model definitions for Neural Networks

**This code defines a class for a feedforward neural network model that consists of an input layer, one or more hiddnen layers, and an output layer. The number of and size of hidden layers, as well as activation functions and other hyperparameters, are all configurable.**

**This setup allows us to customly define a search space of hyperparameters and architectures to loop through and test in our pipeline. I also included options for dropout layers and weight decay for regularization. Since the model is so flexible and complex, these regularization options could help prevent overfitting.**

**The `dropout layer` randomly sets a fraction of the input units to 0 at each update during training, which helps prevent each neuron from becoming too reliant on any one input. 
The `weight decay` parameter is used to apply L2 regularization to the weights of the model, which helps prevent overfitting by penalizing large weights.**

```{python, eval=FALSE}

import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dims, output_dim, activation='ReLU', dropout=0.0):
        super(SimpleNN, self).__init__()
        if isinstance(hidden_dims, int):
            hidden_dims = [hidden_dims]
        if isinstance(activation, str):
            activations = [activation] * len(hidden_dims)
        else:
            activations = activation
        layers = []
        prev_dim = input_dim
        for h_dim, act in zip(hidden_dims, activations):
            layers.append(nn.Linear(prev_dim, h_dim))
            act_layer = getattr(nn, act)() if hasattr(nn, act) else nn.ReLU()
            layers.append(act_layer)
            if dropout > 0.0:
                layers.append(nn.Dropout(dropout))
            prev_dim = h_dim
        layers.append(nn.Linear(prev_dim, output_dim))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

```

### Cross-Validation Loop

**Looping through each of the hyperparameters and architectures is computationally expensive, so I decided to convert our training data into a .parquet file for faster loading times. This significantly sped up the training process.**

**In this code, we can also easily define the target as `Duration_In_Min` or `Occupancy` depending on which model we want to train. In a separate code chunk, I can change the name of the saved output file to reflect the target being trained/tested.**

**I also saved a houldout_indices.npy file for consistent holdout sampling and fair comparisons between models in the end.**

**In the code below, the `search_space` variable is a dictionary that defines all the hyperparameters and their possible values to be tested.**

```{python, eval=FALSE}

def main():
    df = load_data('data/processed/train_engineered.parquet')
    BASE_FEATURES_TO_DROP = [
        'Student_IDs', 'Semester', 'Class_Standing', 'Major',
        'Expected_Graduation', 'Course_Name', 'Course_Number',
        'Course_Type', 'Course_Code_by_Thousands', 'Check_Out_Time',
        'Session_Length_Category', 'Check_In_Date', 'Semester_Date',
        'Expected_Graduation_Date',
        'Duration_In_Min', 'Occupancy'
    ]

    target = 'Occupancy'

    drop_cols = list(set(BASE_FEATURES_TO_DROP) - {target})
    X, y = preprocess_data(df, target, drop_cols)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    holdout_indices_path = 'data/processed/holdout_indices.npy'
    X_non_holdout, y_non_holdout, X_holdout, y_holdout = get_holdout_split(X_scaled, y, holdout_indices_path)

    search_space = {
        "n_layers": [1, 2, 3],
        "n_units_l0": [12, 50, 100, 150],
        "n_units_l1": [12, 50, 100],
        "n_units_l2": [12, 50],
        "activation": ["ReLU"],
        "learning_rate": [0.01],
        "batch_size": [2048],
        "dropout": [0, 0.2, 0.3],
        "weight_decay": [0, 1e-5, 1e-4]
    }

    # ... Loop through all combinations of hyperparameters ...

    # ... Printing and saving results ...

```

### Holdout Evaluation

**After each of the model configurations were tested using 5 Fold cross-validation, each combination was also fit to all non-holdout data and used to predict the holdout data. Although we might usually only refit and test the best-performing model from cross-validation, I suspected the larger architectures would dominate during this process and risk overfitting. Thus, I included all the models so the simpler architectures would also have a chance. The results were saved to a .csv file for easy comparison and analysis.**

**The following code is a simplified version of the retraining and evaluation process Where RMSE, MAE, and R2 metrics are calculated.**

```{python, eval=FALSE}

def retrain_and_evaluate_on_holdout(best_params, X_non_holdout, y_non_holdout, X_holdout, y_holdout):

    # ... verbose for debugging ...
    # ... refit each model on the non-holdout data ...

    model.eval()
    preds, targets_list = [], []
    with torch.no_grad():
        for features, targets in val_loader:
            features, targets = features.to(device), targets.to(device)
            outputs = model(features)
            preds.append(outputs.cpu().numpy())
            targets_list.append(targets.cpu().numpy())
    preds_concat = np.concatenate(preds, axis=0)
    targets_concat = np.concatenate(targets_list, axis=0)
    rmse = np.sqrt(mean_squared_error(targets_concat, preds_concat))
    mae = np.mean(np.abs(targets_concat - preds_concat))
    ss_res = np.sum((targets_concat - preds_concat) ** 2)
    ss_tot = np.sum((targets_concat - np.mean(targets_concat)) ** 2)
    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else float('nan')
    print(f"Holdout Results: RMSE={rmse:.4f} | MAE={mae:.4f} | R2={r2:.4f}")
    return rmse, mae, r2

```

### Results

**The results of both the cross validation and holdout evaluations were surprisingly underwhelming. None of the model configurations seemed perform better than our previous XGBoost, MARS, or Random Forest models. So, they will not be used for our final predictions, but we will include two of the model configurations as part of our required 5 models for the project.**

**Architectures, hyperparameters, and resulting RMSE scores can be seen in the table below.**

#### Neural Networks for Duration

| Model Name      | # Layers | Hidden Units      | Activation(s) | Dropout | Weight Decay | CV RMSE   | Holdout RMSE |
|:--------------- |:--------:|:-----------------:|:-------------:|:-------:|:------------:|:---------:|:------------:|
| NeuralNet-1     |    3     | [100 > 50 > 50]   | ReLU          | 0.3     | 1e-4         | 59.67     | 62.02        |
| NeuralNet-2     |    3     | [150 > 50 > 50]   | ReLU          | 0.3     | 1e-4         | 59.59     | 62.05        |


## Final Model Selection and Weighting

**As mentioned previously, random forests performs the best of our attempted models, and so we choose it to model Duration_In_Min. We now collect the training data together and cross-validate to select an optimal weight for the model. The final tuning parameter values were mtry = 20 (20 predictors used when considering a split), trees = 200, and min_n = 10 (splits only occur at nodes that have at least 10 observations); these were chosen through a more extensive grid search than what is shown above, but we note that this more extensive grid search was performed for MARS, random forests, and XGBoost, and still ultimately favored random forests.**

``` {r, warning = FALSE}
# To select w1/w2 via final CV

studentsfull <- read.csv(here::here("data/processed/train_engineered.csv"))
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
testInd <- createFolds(maintrain$Duration_In_Min, k = 5)

IDLIST <- list()
CVMSE <- c()

tune <- data.frame(mtry = 20)
for (i in 1:5) {
  fullF <- train(Duration_In_Min ~ ., data = maintrain[-testInd[[i]],], method = "rf",
                 tuneGrid = tune, ntree = 200, nodesize = 10)
  fittedF <- predict(fullF, newdata = maintrain[testInd[[i]],])
  IDLIST[[i]] <- data.frame(Fold = i,
                            ids = studentsfull$Student_IDs[testInd[[i]]],
                            fittedF,
                            TRUEy = studentsfull$Duration_In_Min[testInd[[i]]])
}

stopCluster(cl)
```

``` {r}
ID <- do.call(rbind, IDLIST)
colnames(ID) <- c("Fold", "ids", "fittedF", "TRUEy")

ID$AveOtherVisits <- NA
for (i in 1:nrow(ID)) {
  current_id <- ID$ids[i]
  current_fold <- ID$Fold[i]
  
  #Return predictions for the same ID in other folds
  mask <- ID$ids == current_id & ID$Fold != current_fold
  ID$AveOtherVisits[i] <- mean(ID$TRUEy[mask])
}


weights <- c(0, 1/6, 1/4, 1/3, 1/2, 2/3, 3/4, 5/6, 1)
RMSE <- c()
for (j in 1:length(weights)) {
  w1 <- weights[j]
  w2 <- 1-w1
  ID$Weighted <- ifelse(is.nan(ID$AveOtherVisits), ID$fittedF,
                        w1*ID$fittedF + w2*ID$AveOtherVisits)
  CVMSEW <- c()
  for (i in 1:5) {
    CVMSEW[i] <- sum((ID$Weighted[testInd[[i]]] -
                     ID$TRUEy[testInd[[i]]])^2)/length(testInd[[i]])
  }
  RMSE[j] <- sqrt(sum(CVMSEW)/5)
}

data.frame(weights, RMSE)
```

**Trying a grid of weights, we see that a weight of 3/4 towards our predicted values and a weight of 1/4 towards the mean of a student's training data visit durations is best. Our final predictions for Duration are created below.**

``` {r}
modpreds <- read.csv(here::here("data/predictions/Duration_In_Min_RandomForest_20250423180825_pred.csv"))
fulltest <- read.csv(here::here("data/processed/test_engineered.csv"))

predIDs <- data.frame(modpreds, ids = fulltest$Student_IDs)

predIDs$adj <- NA
for (i in 1:nrow(predIDs)) {
  mask <- ID$ids == predIDs$ids[i]
  predIDs$adj[i] <- mean(ID$TRUEy[mask])
}

w1 <- 3/4
w2 <- 1/4
predIDs$FINAL <- ifelse(is.nan(predIDs$adj), predIDs$.pred,
                           w1*predIDs$.pred + w2*predIDs$adj)
write.csv(predIDs$FINAL, "../data/predictions/Duration_FINAL", row.names = F)
```



# Part 2: Predicting Occupancy from Learning Commons Data

## MARS, Random Forests, and Boosting

**It is worth noting that Occupancy is not expected to be a function of individual student behavior in the same way Duration is, so we do not employ post-prediction weighting of any kind here.**

``` {r, echo = FALSE}
# --- Load Libraries ---
# Using suppressPackageStartupMessages to minimize console noise
suppressPackageStartupMessages(library(here)) # For locating files relative to project root
suppressPackageStartupMessages(library(dplyr)) # For data manipulation
suppressPackageStartupMessages(library(rsample)) # For data splitting and CV folds
suppressPackageStartupMessages(library(tune)) # For hyperparameter tuning
suppressPackageStartupMessages(library(yardstick)) # For model evaluation metrics
suppressPackageStartupMessages(library(workflows)) # For combining recipes and models
suppressPackageStartupMessages(library(parsnip)) # For model specifications
suppressPackageStartupMessages(library(recipes)) # For preprocessing
suppressPackageStartupMessages(library(readr)) # For reading/writing data
suppressPackageStartupMessages(library(glue)) # For formatted string output

# --- Source Helper Functions ---
# Construct full paths using here() for robustness
source(here::here("src/r/utils/data_utils.R"))
source(here::here("src/r/recipes/recipes.R"))
source(here::here("src/r/models/models.R"))
source(here::here("src/r/workflows/workflows.R"))
source(here::here("src/r/tuning/tuning.R"))
source(here::here("src/r/training/training.R"))
source(here::here("src/r/evaluation/evaluation.R"))

# --- Configuration ---
# TODO: Move these to a config file/package (e.g., config::get())
DATA_FILENAME <- "train_engineered.csv" # Use only the training data
TEST_SPLIT_PROP <- 0.8 # Proportion of data for training set
TARGET_VARIABLE <- "Occupancy"
CV_FOLDS <- 5 # Number of cross-validation folds
SEED <- 3 # For reproducibility
TUNING_METRIC <- "rmse" # Metric to select best hyperparameters (use "accuracy" or "roc_auc" for classification)
# Features to drop (matching Python preprocess.py, excluding targets and date columns handled in recipe)
FEATURES_TO_DROP <- c(
    "Student_IDs", "Semester", "Class_Standing", "Major", "Expected_Graduation",
    "Course_Name", "Course_Number", "Course_Type", "Course_Code_by_Thousands",
    "Check_Out_Time", "Session_Length_Category"
) # Add others as needed

# Define metric set based on task (regression in this case)
# TODO: Handle classification metrics if TARGET_VARIABLE changes
MODEL_METRICS <- metric_set(rmse, rsq, mae)


# --- Pipeline Execution ---

set.seed(SEED)

# 1. Load Data
cat(glue::glue("--- Loading Data ({DATA_FILENAME}) ---\n\n"))
full_data <- load_data(DATA_FILENAME)
cat(glue::glue("Full data rows: {nrow(full_data)}\n\n"))

# 2. Split Data into Training and Testing
cat(glue::glue("--- Splitting Data (Train: {TEST_SPLIT_PROP*100}%, Test: {(1-TEST_SPLIT_PROP)*100}%) ---\n\n"))
data_split <- rsample::initial_split(full_data, prop = TEST_SPLIT_PROP, strata = NULL) # Add strata if needed
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)
cat(glue::glue("Training data rows: {nrow(train_data)}, Test data rows: {nrow(test_data)}\n\n"))

# 3. Create Recipe
cat(glue::glue("--- Creating Recipe for Target: {TARGET_VARIABLE} ---\n\n"))
# NOTE: FEATURES_TO_DROP list is now correctly defined above
recipe_obj <- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)
#print(recipe_obj) # Print summary of recipe steps

prepped_recipe <- prep(recipe_obj, training = full_data)
maintrainOCC <- bake(prepped_recipe, new_data = full_data)
```

``` {r, echo = FALSE}
testsizeO <- floor(0.2*nrow(maintrainOCC))
set.seed(7560)
hINDO <- sample(1:nrow(maintrainOCC), size = testsizeO)
studentsO <- maintrainOCC[-hINDO,]
holdoutO <- maintrainOCC[hINDO,]

studentsfullO <- read.csv(here::here("data/processed/train_engineered.csv"))
trainfullO <- studentsfullO[-hINDO,]
holdoutfullO <- studentsfullO[hINDO,]
```

``` {r}
#Cleanup
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}

unregister_dopar()
```

``` {r}
#MARS

set.seed(7560)
tc1O <- trainControl(method = "cv", number = 5, search = "grid")
full1O <- train(Occupancy ~ ., data = studentsO, method = "earth",
               trControl = tc1O, tuneLength = 5)
hfit1O <- predict(full1O, newdata = holdoutO)
toterror1O <- sum((hfit1O - holdoutO$Occupancy)^2)/length(hfit1O)
sqrt(toterror1O)
```

**MARS (with a basic 'tuneLength' grid search for hyperparameters) yields an RMSE of 3.75, performing slightly worse than the penalized spline model we selected last semester, which had an RMSE of 3.64.**

``` {r, warning = FALSE}
#Random forests
library(doParallel)

num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

set.seed(7560)
tc2O <- trainControl(method = "cv", number = 5, search = "grid")
full2O <- train(Occupancy ~ ., data = studentsO, method = "rf",
               trControl = tc2O, tuneLength = 3)
hfit2O <- predict(full2O, newdata = holdoutO)
toterror2O <- sum((hfit2O - holdoutO$Occupancy)^2)/length(hfit2O)
sqrt(toterror2O)

stopCluster(cl)
```

**Random forests yields markedly better performance, with an RMSE of 1.83.**

``` {r}
#Boosting
num_cores <- detectCores(logical = FALSE)
num_cores_to_use <- num_cores - 4
cl <- makePSOCKcluster(num_cores_to_use)
registerDoParallel(cl)

tc3O <- trainControl(method = "cv", number = 5, search = "grid")
full3O <- train(Occupancy ~ ., data = studentsO, method = "xgbTree",
               trControl = tc3O, tuneLength = 3, verbose = FALSE)
hfit3O <- predict(full3O, newdata = holdoutO)
toterror3O <- sum((hfit3O - holdoutO$Occupancy)^2)/length(hfit3O)
sqrt(toterror3O)

stopCluster(cl)
```

**XGBoost performs better than MARS and better than the best models we tried last semester (its RMSE is about 2.52), but the clear winner among the models tried thus far is random forests.**

## Neural Networks for Occupancy

**The discussion on neural networks given for duration is essentially the same as for occupancy. The final results for these models are summarized below:**

| Model Name      | # Layers | Hidden Units      | Activation(s) | Dropout | Weight Decay | CV RMSE   | Holdout RMSE |
|:--------------- |:--------:|:-----------------:|:-------------:|:-------:|:------------:|:---------:|:------------:|
| NeuralNet-1     |    1     | [150]             | ReLU          | 0.2     | 1e-5         | 3.62      | 3.17         |
| NeuralNet-2     |    1     | [100]             | ReLU          | 0.2     | 1e-4         | 3.67      | 3.23         |

## Final Model Selection

**Random forests maintained its superior performance over neural networks for the occupancy problem, so we selected it as our final model. No weighting was applied here (as individual student behaviors do not have as obvious of an intuitive role in Learning Commons occupancy). The hyperparameters chosen via cross-validation that we settled on for our random forest model were mtry = 20, trees = 200, and min_n = 2, a slightly more flexible model than that used for duration (since min_n is smaller, allowing splits to occur even in nodes with only 2 observations).**

**In both the duration and occupancy problems, we have seen that random forests, while computationally costly, yields slight improvements on predictive power both compared to the other non-linear models tried here and compared to the linear ones tried last semester, at least in cross-validation and holdout analysis. Predictions for both visit duration and occupancy appear to benefit from allowing for non-linear effects in our model.**

``` {r}
modpredsO <- read.csv(here::here("data/predictions/Occupancy_RandomForest_20250423155038_pred.csv"))
colnames(modpredsO) <- "x"
write.csv(modpredsO, "../data/predictions/Occupancy_FINAL", row.names = F)
```
