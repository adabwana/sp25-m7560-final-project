{"title":"Neural Networks","markdown":{"headingText":"Neural Networks","containsRefs":false,"markdown":"\nIn addition to the XGBoost, MARS, and Random Forest models described in the previous sections, we also wanted to explore how neural networks would perform on our two tasks. \n\nIn this section, we will construct a simple feedforward neural network in Python using the PyTorch library. As there are endless possible combinations of architectures paired with different hyperparameters, we will use 5 Fold cross validation to train/validate a subset of possible models. We also sampled a set of holdout test data to evaluate the overall performances of each model on data not seen during training.\n\nTo avoid copying an overwhelming amount of python code in our report, we've only included simplified code chunks to showcase our model training and evaluation processes.\n\n*Note: In this section, we will not incorporate the post-hoc visit weighting scheme we implemented in the previous sections.* \n\n\n## Model Definition \n\nThis code defines a class for a feedforward neural network model that consists of an input layer, one or more hiddnen layers, and an output layer. The number of and size of hidden layers, as well as activation functions and other hyperparameters, are all configurable.\n\nThis setup allows us to customly define a search space of hyperparameters and architectures to loop through and test in our pipeline. I also included options for dropout layers and weight decay for regularization. Since the model is so flexible and complex, these regularization options could help prevent overfitting.\n\nThe `dropout layer` randomly sets a fraction of the input units to 0 at each update during training, which helps prevent each neuron from becoming too reliant on any one input. \nThe `weight decay` parameter is used to apply L2 regularization to the weights of the model, which helps prevent overfiting by penalizing large weights.\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, activation='ReLU', dropout=0.0):\n        super(SimpleNN, self).__init__()\n        if isinstance(hidden_dims, int):\n            hidden_dims = [hidden_dims]\n        if isinstance(activation, str):\n            activations = [activation] * len(hidden_dims)\n        else:\n            activations = activation\n        layers = []\n        prev_dim = input_dim\n        for h_dim, act in zip(hidden_dims, activations):\n            layers.append(nn.Linear(prev_dim, h_dim))\n            act_layer = getattr(nn, act)() if hasattr(nn, act) else nn.ReLU()\n            layers.append(act_layer)\n            if dropout > 0.0:\n                layers.append(nn.Dropout(dropout))\n            prev_dim = h_dim\n        layers.append(nn.Linear(prev_dim, output_dim))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n```\n\n## Cross-Validation Loop\n\nLooping through each of the hyperparameters and architectures is computationally expensive, so I decided to convert our training data into a .parquet file for faster loading times. This significantly sped up the training process.\n\nIn this code, we can also easily define the target as `Duration_In_Min` or `Occupancy` depending on which model we want to train. In a separate code chunk, I can change the name of the saved output file to reflect the target being trained/tested. \n\nI also saved a houldout_indices.npy file for consistent holdout sampling and fair comparisons between models in the end.\n\nIn the code below, the `search_space` variable is a dictionary that defines all the hyperparameters and their possible values to be tested.\n\n```python\ndef main():\n    df = load_data('data/processed/train_engineered.parquet')\n    BASE_FEATURES_TO_DROP = [\n        'Student_IDs', 'Semester', 'Class_Standing', 'Major',\n        'Expected_Graduation', 'Course_Name', 'Course_Number',\n        'Course_Type', 'Course_Code_by_Thousands', 'Check_Out_Time',\n        'Session_Length_Category', 'Check_In_Date', 'Semester_Date',\n        'Expected_Graduation_Date',\n        'Duration_In_Min', 'Occupancy'\n    ]\n\n    target = 'Occupancy'\n\n    drop_cols = list(set(BASE_FEATURES_TO_DROP) - {target})\n    X, y = preprocess_data(df, target, drop_cols)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    holdout_indices_path = 'data/processed/holdout_indices.npy'\n    X_non_holdout, y_non_holdout, X_holdout, y_holdout = get_holdout_split(X_scaled, y, holdout_indices_path)\n\n    search_space = {\n        \"n_layers\": [1, 2, 3],\n        \"n_units_l0\": [12, 50, 100, 150],\n        \"n_units_l1\": [12, 50, 100],\n        \"n_units_l2\": [12, 50],\n        \"activation\": [\"ReLU\"],\n        \"learning_rate\": [0.01],\n        \"batch_size\": [2048],\n        \"dropout\": [0, 0.2, 0.3],\n        \"weight_decay\": [0, 1e-5, 1e-4]\n    }\n\n    # ... Loop through all combinations of hyperparameters ...\n\n    # ... Printing and saving results ...\n```\n\n## Holdout Evaluation\n\nAfter each of the model configurations were tested using 5 Fold cross-validation, each combination was also fit to all non-holdout data and used to predict the holdout data. Although we might usually only refit and test the best-performing model from cross-validation, I suspected the larger architectures would dominate during this process and risk overfitting. Thus, I included all the models so the simpler architectures would also have a chance. The results were saved to a .csv file for easy comparison and analysis.\n\nThe following code is a simplified version of the retraining and evaluation process Where RMSE, MAE, and R2 metrics are calculated.\n\n```python\ndef retrain_and_evaluate_on_holdout(best_params, X_non_holdout, y_non_holdout, X_holdout, y_holdout):\n\n    # ... verbose for debugging ...\n    # ... refit each model on the non-holdout data ...\n\n    model.eval()\n    preds, targets_list = [], []\n    with torch.no_grad():\n        for features, targets in val_loader:\n            features, targets = features.to(device), targets.to(device)\n            outputs = model(features)\n            preds.append(outputs.cpu().numpy())\n            targets_list.append(targets.cpu().numpy())\n    preds_concat = np.concatenate(preds, axis=0)\n    targets_concat = np.concatenate(targets_list, axis=0)\n    rmse = np.sqrt(mean_squared_error(targets_concat, preds_concat))\n    mae = np.mean(np.abs(targets_concat - preds_concat))\n    ss_res = np.sum((targets_concat - preds_concat) ** 2)\n    ss_tot = np.sum((targets_concat - np.mean(targets_concat)) ** 2)\n    r2 = 1 - ss_res / ss_tot if ss_tot > 0 else float('nan')\n    print(f\"Holdout Results: RMSE={rmse:.4f} | MAE={mae:.4f} | R2={r2:.4f}\")\n    return rmse, mae, r2\n```\n\n## Results\n\nThe results of both the cross validation and holdout evaluations were surprisingly underwhelming. None of the model configurations seemed perform better than our previous XGBoost, MARS, or Random Forest models. So, they will not be used for our final predictions, but we will include two of the model configurations as part of our required 5 models for the project.\n\nArchitectures, hyperparameters, and resulting RMSE scores can be seen in the table below.\n\n### **Duration Models**\n\n| Model Name      | # Layers | Hidden Units      | Activation(s) | Dropout | Weight Decay | CV RMSE   | Holdout RMSE |\n|:--------------- |:--------:|:-----------------:|:-------------:|:-------:|:------------:|:---------:|:------------:|\n| NeuralNet-1     |    3     | [100 > 50 > 50]   | ReLU          | 0.3     | 1e-4         | 59.67     | 62.02        |\n| NeuralNet-2     |    3     | [150 > 50 > 50]   | ReLU          | 0.3     | 1e-4         | 59.59     | 62.05        |\n\n### **Occupancy Models**\n\n| Model Name      | # Layers | Hidden Units      | Activation(s) | Dropout | Weight Decay | CV RMSE   | Holdout RMSE |\n|:--------------- |:--------:|:-----------------:|:-------------:|:-------:|:------------:|:---------:|:------------:|\n| NeuralNet-1     |    1     | [150]             | ReLU          | 0.2     | 1e-5         | 3.62      | 3.17         |\n| NeuralNet-2     |    1     | [100]             | ReLU          | 0.2     | 1e-4         | 3.67      | 3.23         |\n\n***\n","srcMarkdownNoYaml":""},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"output-file":"neural_networks.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.43","theme":"united"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}