[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "M7560: Final Project",
    "section": "",
    "text": "Predicting Learning Commons Usage\nSpring 2025 | Math 75650 Statistical Learning II | Date: 4/26/25\nBy: Emma Naiyue Liang, Ryan Renken, Jaryt Salvo, & Jason Turk\nView our code base on GitHub",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#project-overview",
    "href": "index.html#project-overview",
    "title": "M7560: Final Project",
    "section": "Project Overview",
    "text": "Project Overview\nThis project develops predictive models for student usage patterns at the BGSU Learning Commons (LC). We focus on two specific prediction challenges: estimating visit duration and forecasting building occupancy at the time of check-in. Different modeling approaches were tailored to each task.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#data-architecture",
    "href": "index.html#data-architecture",
    "title": "M7560: Final Project",
    "section": "Data Architecture",
    "text": "Data Architecture\nThe analysis uses LC visit data spanning two academic years. Models were trained on data from Fall 2016 - Spring 2017. Our validation framework utilized a reserved portion of this 2016-2017 data as a holdout set to assess model generalization and stability. Features included student demographics, academic metrics (course load, GPA), temporal information (time of day, week of semester), and external data (weather, lunar phase). An observed bias towards senior-class representation in the data was noted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#methodological-framework",
    "href": "index.html#methodological-framework",
    "title": "M7560: Final Project",
    "section": "Methodological Framework",
    "text": "Methodological Framework\nWe employed a structured approach using the R tidymodels ecosystem. Feature engineering was performed using the recipes package to create informative predictors from raw data, including transformations for temporal features and standardization of numeric variables.\nFor both prediction tasks (duration and occupancy), we evaluated several algorithms, including Multivariate Adaptive Regression Splines (MARS), Random Forest, XGBoost, Multi-Layer Perceptron (MLP) networks, and Gated Recurrent Unit (GRU) networks. Hyperparameters for each model were optimized using 5-fold cross-validation specific to the prediction task.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#implementation-results",
    "href": "index.html#implementation-results",
    "title": "M7560: Final Project",
    "section": "Implementation Results",
    "text": "Implementation Results\nModel performance was assessed on a dedicated holdout dataset (a reserved portion of the Fall 2016 - Spring 2017 data) using Root Mean Squared Error (RMSE) and R-squared (R²).\n\nDuration Prediction: This proved challenging. The best model (XGBoost) achieved an RMSE of 59.9 minutes and an R² of 0.099. Performance was slightly improved by incorporating a weighted average with the training set mean duration.\nOccupancy Prediction: This yielded substantially better results. The optimized XGBoost model achieved an RMSE of 1.83 students and an R² of 0.911, demonstrating strong predictive power.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#technical-infrastructure",
    "href": "index.html#technical-infrastructure",
    "title": "M7560: Final Project",
    "section": "Technical Infrastructure",
    "text": "Technical Infrastructure\nThe project was implemented in R, primarily utilizing the tidymodels suite (recipes, parsnip, rsample, tune, workflows, yardstick). Model implementations relied on earth (MARS), ranger (Random Forest), and xgboost (XGBoost). Standard packages like dplyr, readr, and ggplot2 were used for data handling and visualization. External data integration used lunar and openmeteo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#research-findings",
    "href": "index.html#research-findings",
    "title": "M7560: Final Project",
    "section": "Research Findings",
    "text": "Research Findings\nPredicting individual visit duration is inherently difficult, likely due to high variability and skewness in usage patterns, resulting in a low R² (0.10). In contrast, predicting building occupancy was highly successful (R² = 0.91). The features and XGBoost model effectively captured the aggregate patterns influencing concurrent LC usage based on check-in data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#future-research-directions",
    "href": "index.html#future-research-directions",
    "title": "M7560: Final Project",
    "section": "Future Research Directions",
    "text": "Future Research Directions\nFuture work could explore integrating additional environmental factors, applying more complex non-linear or time-series specific models, or developing ensemble methods that combine predictions from multiple models to potentially improve performance, particularly for the duration task.\nImplementation details and comprehensive analysis available in associated documentation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "M7560: Final Project",
    "section": "Overview",
    "text": "Overview\nThis section details a comparative analysis between standard K-means and K-means++ initialization methods for clustering.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#data-task",
    "href": "index.html#data-task",
    "title": "M7560: Final Project",
    "section": "Data & Task",
    "text": "Data & Task\nThe analysis was performed on a dataset consisting of \\(\\sim 5000\\) points sampled from a subset of \\(\\mathbb{R}^2\\). The objective was to partition this data into \\(k=11\\) distinct clusters.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#methodology-comparison",
    "href": "index.html#methodology-comparison",
    "title": "M7560: Final Project",
    "section": "Methodology Comparison",
    "text": "Methodology Comparison\nWe compared the performance of standard K-means clustering against K-means utilizing the K-means++ initialization strategy. The K-means++ algorithm aims to select more strategic initial centroids to potentially improve clustering quality and convergence speed compared to random initialization.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#results",
    "href": "index.html#results",
    "title": "M7560: Final Project",
    "section": "Results",
    "text": "Results\n\nStandard K-means: Converged in 5 iterations with a final Within-Cluster Sum of Squares (WCSS) of 22,824.\nK-means++ Initialization: Converged in 8 iterations with a final WCSS of 22,943.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "index.html#findings",
    "href": "index.html#findings",
    "title": "M7560: Final Project",
    "section": "Findings",
    "text": "Findings\nFor this specific \\(\\mathbb{R}^2\\) dataset and \\(k=11\\), the K-means++ initialization method did not demonstrate a discernible advantage over the standard K-means approach, either visually or based on the WCSS metric. The standard method converged faster and achieved a slightly lower WCSS.\nVisualizations and further details available in associated documentation.\n\nsource: src/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Predicting Learning Commons Usage</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html",
    "href": "notebooks/report/feature_engineering.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Temporal Feature Engineering\nThe complete feature engineering implementation can be found in our source code.\nOur feature engineering process began with temporal data extraction using the lubridate and hms packages.\nFrom these validated timestamps, we construct several temporal features:\nAnalysis of visit patterns revealed a non-linear relationship between Check_In_Hour and Duration variables. This observation prompted the creation of a more nuanced Time_Category variable with distinct periods:\nThe Semester and Expected_Graduation variables presented a dimensionality challenge due to their categorical semester format (e.g., “Fall 2016”). We first converted these strings into actual date objects representing the start of the semester/expected graduation semester.\nUsing these dates, we then calculated a numeric ‘Months_Until_Graduation’ metric, effectively reducing complexity while maintaining predictive potential.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#temporal-feature-engineering",
    "href": "notebooks/report/feature_engineering.html#temporal-feature-engineering",
    "title": "Feature Engineering",
    "section": "",
    "text": "prepare_dates &lt;- function(df) {\n df %&gt;% mutate(\n   Check_In_Date = mdy(Check_In_Date),\n   Check_In_Time = hms::as_hms(Check_In_Time)\n )\n}\n\nadd_temporal_features &lt;- function(df) {\n df %&gt;% mutate(\n    Check_In_Day = wday(Check_In_Date, label = TRUE),\n    Is_Weekend = Check_In_Day %in% c(\"Sat\", \"Sun\"),\n    Check_In_Week = ceiling(day(Check_In_Date) / 7),\n    Check_In_Month = month(Check_In_Date, label = TRUE),\n    Check_In_Hour = hour(Check_In_Time)\n )\n}\n\nadd_time_category &lt;- function(df) {\n df %&gt;% mutate(\n   Time_Category = case_when(\n       hour(Check_In_Time) &lt; 6 ~ \"Late Night\",\n       hour(Check_In_Time) &lt; 12 ~ \"Morning\",\n       hour(Check_In_Time) &lt; 17 ~ \"Afternoon\",\n       hour(Check_In_Time) &lt; 22 ~ \"Evening\",\n       TRUE ~ \"Late Night\"\n   )\n )\n}\n\nconvert_semester_to_date &lt;- function(semester_str) {\n  # Extract year and semester\n  parts &lt;- strsplit(semester_str, \" \")[[1]]\n  year &lt;- parts[length(parts)]\n  semester &lt;- parts[1]\n  \n  # Map semesters to months\n  month &lt;- case_when(\n    semester == \"Fall\" ~ \"08\",\n    semester == \"Spring\" ~ \"01\",\n    semester == \"Summer\" ~ \"06\",\n    semester == \"Winter\" ~ \"12\",\n    TRUE ~ NA_character_\n  )\n  \n  # Combine into date (day is arbitrary, set to 01)\n  paste0(month, \"/\", \"01\", \"/\", year)\n}\n\nadd_date_features &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      # Convert semester to date\n      Semester_Date = mdy(purrr::map_chr(Semester, convert_semester_to_date)),\n      # Convert expected graduation to date\n      Expected_Graduation_Date = mdy(purrr::map_chr(Expected_Graduation, convert_semester_to_date)),\n    )\n}\n\nadd_graduation_features &lt;- function(df) {\n  df %&gt;% mutate(\n    # Calculate months until graduation\n    Months_Until_Graduation = as.numeric(\n      difftime(Expected_Graduation_Date, Semester_Date, units = \"days\") / 30.44 # average days per month\n    )\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-related-features",
    "href": "notebooks/report/feature_engineering.html#course-related-features",
    "title": "Feature Engineering",
    "section": "Course-Related Features",
    "text": "Course-Related Features\nThe Course_Code_by_Thousands variable was used to create a Course_Level feature:\nadd_course_features &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Level = case_when(\n      Course_Code_by_Thousands &lt;= 100 ~ \"Special\",\n      Course_Code_by_Thousands &lt;= 3000 ~ \"Lower Classmen\",\n      Course_Code_by_Thousands &lt;= 4000 ~ \"Upper Classmen\",\n      TRUE ~ \"Graduate\" # Includes codes &gt; 4000\n    )\n  )\n}\nTo capture academic performance context, we developed categorical features for Cumulative_GPA and Term_Credit_Hours:\nadd_gpa_category &lt;- function(df) {\n  df %&gt;% mutate(\n    GPA_Category = case_when(\n      Cumulative_GPA &gt;= 3.5 ~ \"Excellent\",\n      Cumulative_GPA &gt;= 3.0 ~ \"Good\",\n      Cumulative_GPA &gt;= 2.0 ~ \"Satisfactory\",\n      TRUE ~ \"Needs Improvement\" # Includes NA GPA values implicitly\n    )\n  )\n}\nadd_credit_load_category &lt;- function(df) {\n  df %&gt;% mutate(\n    # Credit load features\n    Credit_Load_Category = case_when(\n      Term_Credit_Hours &lt;= 6 ~ \"Part Time\",\n      Term_Credit_Hours &lt;= 12 ~ \"Half Time\",\n      Term_Credit_Hours &lt;= 18 ~ \"Full Time\",\n      TRUE ~ \"Overload\" # Includes &gt; 18 hours\n    ),\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#student-classification-features",
    "href": "notebooks/report/feature_engineering.html#student-classification-features",
    "title": "Feature Engineering",
    "section": "Student Classification Features",
    "text": "Student Classification Features\nThe dataset exhibited an unexpected concentration of ‘Senior’ classifications in the original Class_Standing variable. Further investigation suggested this might stem from students accumulating excess credits for senior status without necessarily being in their final year. To address this potential ambiguity while preserving useful information, we implemented a dual classification approach.\nFirst, we recoded the original Class_Standing variable, preserving potentially valuable self-reported information as Class_Standing_Self_Reported.\nadd_class_standing_category &lt;- function(df) {\n  df %&gt;% mutate(\n    # Renaming column and values for Class_Standing\n    Class_Standing_Self_Reported = case_when(\n      Class_Standing == \"Freshman\" ~ \"First Year\",\n      Class_Standing == \"Sophomore\" ~ \"Second Year\",\n      Class_Standing == \"Junior\" ~ \"Third Year\",\n      Class_Standing == \"Senior\" ~ \"Fourth Year\",\n      TRUE ~ Class_Standing # Keeps 'Graduate', 'Other', etc.\n    ),\n  )\n}\nComplementing this, we developed a more objective BGSU Standing metric (Class_Standing_BGSU) based strictly on earned credit hours, following official university definitions. This dual approach provides both self-reported and objective perspectives.\nadd_class_standing_bgsu &lt;- function(df) {\n  df %&gt;% mutate(\n    # Class_standing by BGSU's definition\n    # https://www.bgsu.edu/academic-advising/student-resources/academic-standing.html\n    Class_Standing_BGSU = case_when(\n      Total_Credit_Hours_Earned &lt; 30 ~ \"Freshman\",\n      Total_Credit_Hours_Earned &lt; 60 ~ \"Sophomore\",\n      Total_Credit_Hours_Earned &lt; 90 ~ \"Junior\",\n      Total_Credit_Hours_Earned &lt;= 120 ~ \"Senior\",\n      TRUE ~ \"Extended\" # &gt; 120 credits\n    ),\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-name-and-type-features",
    "href": "notebooks/report/feature_engineering.html#course-name-and-type-features",
    "title": "Feature Engineering",
    "section": "Course Name and Type Features",
    "text": "Course Name and Type Features\nThe raw Course_Name variable presented significant challenges due to its high cardinality and free-text nature. We implemented a detailed keyword-based categorization system (add_course_name_category) to group courses into meaningful academic domains (e.g., Business, Computer Science, Natural Sciences, Humanities) and types (e.g., Introductory, Intermediate, Advanced, Laboratory, Seminar, Independent Study). This involved using grepl with extensive keyword lists and a structured case_when statement to prioritize specific subject areas before applying level or type classifications. This provides a more manageable and informative feature than the raw course names.\nadd_course_name_category &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Name_Category = case_when(\n      # Handle Non-Course Entries First\n      Course_Name %in% c(\"Course Enrollment\") ~ \"Administrative\",\n\n      # Specific Subject Categories\n      grepl(\"Business|Finance|Accounting|Economics|Marketing|Management|Quantitative|\n      |Taxation|Planning|Organizational|Behavior|Money|Banking|Auditing|Global Economy|\n      |Financial Markets|Selling|Managing Change|Global Strategy\",\n            Course_Name, ignore.case = TRUE) ~ \"Business\",\n      grepl(\"Computer|Programming|Data|Software|Network|Database|Algorithm|\n      |Operating Systems|Analytics|Computing|Application\",\n            Course_Name, ignore.case = TRUE) ~ \"Computer Science\",\n      grepl(\"Mathematics|Calculus|Statistics|Probability|Geometry|Discrete|Algebra|\n      |\\bMath\\b|Quantitative|Analytics|Equations|\\bDesign\\b(?=.*Sample)|\\bDesign\\b(?=.*Experimental)|Game Theory\",\n            Course_Name, ignore.case = TRUE, perl=TRUE) ~ \"Mathematics\",\n      grepl(\"Statics|Dynamics|Engineering|Structural|Manufacturing|Electrical|Electronic|Thermodynamics|\n      |Machine Design|Fluid Power|CAD|BIM|Materials|Modeling|GIS|Geographic Information Systems|\n      |Construction|Electric Circuits|Structures|Concrete|Surveying|Estimating|Cost Control\",\n            Course_Name, ignore.case = TRUE) ~ \"Engineering/Technology\",\n      grepl(\"Physics|Chemistry|Biology|Astronomy|Earth|Environment|Science(?!.*Computer|.*Social|.*Political|.*Family|\n      |.*Food)|Solar System|Sea|Marine|Mechanics|Weather|Climate|Limnology|Ecology|Cosmos|Evolution|Life Through Time|Electricity|Magnetism|Meteorology\",\n            Course_Name, ignore.case = TRUE, perl = TRUE) ~ \"Natural Sciences\",\n      grepl(\"Psychology|Sociology|Anthropology|Social|Cultural|Society|Political|Development|\n      |Sexuality|Government|Minority|Adolescent|Family|Geography|Organizational|Behavior|GIS|\n      |Geographic Information Systems|Corrections|Poverty|Discrimination|Interaction|Women|\n      |Juvenile|Delinquency|Interviewing|Observation|Personality|Victimology|Criminology\",\n            Course_Name, ignore.case = TRUE) ~ \"Social Sciences\",\n      grepl(\"History|Philosophy|Ethics|Literature|Culture|Language|Art|Religion|Music|Moral|\n      |Phonetics|Linguistics|Writing|Composition|Conversation|America|Roman|Drawing|Studio|\n      |Performance|Stage|Recital|Civilizations|Thinking|Ideas|Cinematography|Translation|\n      |Mythology|Hispanic|Modern World|Existentialism|Media|Strategic Communication\",\n            Course_Name, ignore.case = TRUE) ~ \"Humanities\",\n      grepl(\"Education|Teaching|Learning|Childhood|Teacher|Curriculum|Child Development|\n      |Families|Field Experience|Communication Development|Design\",\n            Course_Name, ignore.case = TRUE) ~ \"Education\",\n      grepl(\"Anatomy|Physiology|Nutrition|Biomechanics|Exercise|Sport|Dietetics|Health|\n      |Kinesiology|Nursing|Sexuality|Weight Training|Fitness|Food|Acoustics|Speech|Hearing|Epidemiology\",\n            Course_Name, ignore.case = TRUE) ~ \"Health Sciences\",\n\n      # Course Types (Placed after specific subjects)\n      grepl(\"Laboratory|\\bLab\\b\", Course_Name, ignore.case = TRUE) ~ \"Laboratory\",\n      grepl(\"Seminar|Workshop\", Course_Name, ignore.case = TRUE) ~ \"Seminar\",\n      grepl(\"Independent|Special|Practicum|Internship|Field Experience|Topics\", # Added Topics here\n            Course_Name, ignore.case = TRUE) ~ \"Independent/Applied Study\",\n\n      # Course Levels (Applied last before Other/No Response)\n      grepl(\"Advanced|III|3|Analysis|Senior|Graduate|Dissertation|Research|Capstone\",\n            Course_Name, ignore.case = TRUE) ~ \"Advanced\",\n      grepl(\"Intermediate|II$|II |2|Applied\",\n            Course_Name, ignore.case = TRUE) ~ \"Intermediate\",\n      grepl(\"Basic|Elementary|Intro|Introduction|Fundamental|General|Principles|Orientation|Success\",\n            Course_Name, ignore.case = TRUE) ~ \"Introductory\",\n      grepl(\"No Response\", Course_Name, ignore.case = TRUE) ~ \"No Response\",\n\n      # Default case\n      TRUE ~ \"Other\"\n    )\n  )\n}\nSimilarly, the Course_Type variable (e.g., “MATH”, “HIST”, “ENG”) required consolidation. We grouped the original prefixes into broader academic categories (e.g., “Business & Economics”, “Natural Sciences”, “Humanities & Arts”, “Engineering & Technology”) using case_when, explicitly handling NA and “No Response” values first.\nadd_course_type_category &lt;- function(df) {\n  df %&gt;% mutate(\n    Course_Type_Category = case_when(\n      is.na(Course_Type) ~ \"No Response\", # Handle NA values\n      Course_Type == \"No Response\" ~ \"No Response\", # Handle explicit \"No Response\"\n\n      # Specific Categories based on Course_Type prefix\n      Course_Type %in% c(\"ACCT\", \"BA\", \"ECON\", \"FIN\", \"LEGS\", \"MGMT\", \"MKT\", \"MIS\", \"BIZX\", \"MBA\", \"ORGD\") ~ \"Business & Economics\",\n      Course_Type %in% c(\"BIOL\", \"CHEM\", \"GEOL\", \"PHYS\", \"ASTR\", \"ENVS\", \"SEES\") ~ \"Natural Sciences\",\n      Course_Type %in% c(\"CS\") ~ \"Computer Science\",\n      Course_Type %in% c(\"MATH\") ~ \"Mathematics\",\n      Course_Type %in% c(\"STAT\", \"OR\") ~ \"Statistics\",\n      Course_Type %in% c(\"ART\", \"CDIS\", \"CLAS\", \"COMM\", \"ENG\", \"ETHN\", \"FREN\", \"GER\", \"GERM\", \"HIST\", \"HUM\", \"LAT\", \"PHIL\", \"POPC\", \"SPAN\", \"THFM\", \"CHIN\", \"JAPN\", \"ARTH\", \"MUCH\", \"MDIA\", \"JOUR\", \"AMPD\", \"GSW\", \"MUCT\", \"ID\", \"RUSN\", \"ITAL\", \"MUS\", \"CLCV\") ~ \"Humanities & Arts\", # Added GSW, MUCT, ID, RUSN, ITAL, MUS, CLCV\n      Course_Type %in% c(\"AFS\", \"ROTC\") ~ \"Military Science\",\n      Course_Type %in% c(\"EDAS\", \"EDCI\", \"EDEC\", \"EDFI\", \"EDHD\", \"EDIS\", \"EDL\", \"EDMS\", \"EDTL\", \"HIED\", \"ACEN\", \"EIEC\") ~ \"Education\",\n      Course_Type %in% c(\"SOC\", \"PSYC\", \"POLS\", \"GEOG\", \"JOUR\", \"WMST\", \"WS\", \"ACS\", \"CAST\", \"CRJU\", \"SOWK\", \"GERO\") ~ \"Social Sciences\",\n      Course_Type %in% c(\"HDFS\", \"HNRS\", \"UNIV\", \"RESC\") ~ \"Interdisciplinary/Honors\",\n      Course_Type %in% c(\"FDST\", \"NUTR\", \"EXSC\", \"PE\", \"PUBH\", \"ESHP\", \"FN\", \"SM\", \"NURS\", \"MLS\", \"PEG\", \"AHTH\", \"DHS\", \"HMSL\") ~ \"Health Sciences\",\n      Course_Type %in% c(\"EET\", \"IT\", \"MET\", \"QS\", \"CONS\", \"IS\", \"ENGT\", \"ARCH\", \"ECET\") ~ \"Engineering & Technology\",\n\n      # Default Case\n      TRUE ~ \"Other\" # Catch-all for any unmapped types\n    )\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#major-categories",
    "href": "notebooks/report/feature_engineering.html#major-categories",
    "title": "Feature Engineering",
    "section": "Major Categories",
    "text": "Major Categories\nThe Major variable demanded a similar, extensive keyword-based reduction strategy. We developed a detailed case_when structure using grepl to map numerous specific major names into broader categories like “Business & Management”, “Computer Science/IT”, “Natural Sciences”, “Health Sciences”, “Social Sciences & History”, “Arts & Humanities”, etc. This process explicitly handled undecided/non-degree students and pre-professional tracks.\nCrucially, this function also identifies students with multiple majors (Has_Multiple_Majors) by detecting commas in the Major field (after cleaning out minor designations) and identifies the presence of minors (Has_Minor) using “-MIN”.\nadd_major_category &lt;- function(df) {\n  df %&gt;% mutate(\n    # Create cleaned Major string for multi-major check (remove comma-separated minors)\n    Major_Cleaned = ifelse(is.na(Major), \"\", Major),\n    Major_Cleaned = gsub(\",[^,]+-MIN(:[^,]*)?\", \"\", Major_Cleaned, ignore.case = TRUE),\n    Major_Cleaned = gsub(\"[^,]+-MIN(:[^,]*)?,\", \"\", Major_Cleaned, ignore.case = TRUE),\n    \n    # Check for multiple majors based on cleaned string\n    Has_Multiple_Majors = ifelse(grepl(\",\", Major_Cleaned, fixed = TRUE), TRUE, FALSE),\n    \n    # Check for minor presence in the original string\n    Has_Minor = ifelse(grepl(\"-MIN\", Major, fixed = TRUE, ignore.case = TRUE), TRUE, FALSE),\n    # Is_Pre_Professional = ifelse(grepl(\"Pre-Med|Pre-Vet|Pre-Law|Pre-Dent|Pre-Prof\", Major, ignore.case = TRUE), TRUE, FALSE), # Optional flag if needed elsewhere\n    \n    Major_Category = case_when(\n      # Handle Undecided/Non-Degree First\n      grepl(\"Undecided|Deciding\", Major, ignore.case = TRUE) ~ \"Undecided/Deciding\",\n\n      # Subject Categories (Split CompSci/Eng, added keywords)\n      grepl(\"Education|Teaching|EDTL|EDAS|EDCI|EDEC|EDFI|EDHD|EDIS|EDL|EDMS|BSED|MED|PHD|Childhood|Adolescent|Intervention|Integrated|College Student Personnel|CSP\",\n            Major, ignore.case = TRUE) ~ \"Education\",\n      grepl(\"Business|Finance|Accounting|Economics|Marketing|Management|Supply Chain|SCM|ENTREP|ORGD|BA|BSBA|MBA|BIZX|Tourism|Hospitality|Event|Resort|Attraction|Selling|Human Resource|Sport Management|SPMGT\",\n            Major, ignore.case = TRUE) ~ \"Business & Management\",\n      grepl(\"Computer Science|Software|Data|Information Systems|Computing|Analytics|CS\", # Moved Analytics here for Busn Analytics\n            Major, ignore.case = TRUE) ~ \"Computer Science/IT\",\n      grepl(\"Engineering|ENGT|Technology|Electronics|ECET|EET|MET|CONS|Construction|Aviation|AVST|Architecture|ARCH|Mechatronics|EMST|Manufacturing|Quality Systems|Visual Communication|VCT\",\n            Major, ignore.case = TRUE) ~ \"Engineering & Technology\",\n      grepl(\"Biology|BIOL|Chemistry|CHEM|Physics|PHYS|Geology|GEOL|Astronomy|ASTR|Environment|ENVS|SEES|Marine|Geospatial|Science(?!.*Computer|.*Social|.*Political|.*Family|.*Food|.*Health|.*Sport)\",\n            Major, ignore.case = TRUE, perl=TRUE) ~ \"Natural Sciences\",\n      grepl(\"Mathematics|MATH|Statistics|STAT|Actuarial|ASOR\",\n            Major, ignore.case = TRUE) ~ \"Mathematics & Statistics\",\n      grepl(\"Health|Nursing|NURS|BSNUR|Nutrition|Dietetics|FN|Food|FDST|Kinesiology|Exercise|EXSC|Sport|SM|HMSL|Athletic Training|MAT|Medical Lab|MEDTECH|MLS|Respiratory Care|RCT|Gerontology|GERO|LTCR|Allied Health|AHTH|DHS|Public Health|PUBH|Communication Disorders|CDIS\",\n            Major, ignore.case = TRUE) ~ \"Health Sciences\",\n      grepl(\"Psychology|PSYC|Sociology|SOC|Anthropology|ANTH|Social Work|SOWK|Criminal Justice|CRJU|Political Science|POLS|Geography|GEOG|History|HIST|International|Public Admin|MPA|Family|HDFS|Liberal Arts|PPEL\",\n            Major, ignore.case = TRUE) ~ \"Social Sciences & History\", # Combined History here\n      grepl(\"Art|BFA|Music|MUS|MUCT|Theatre|THFM|Film|Media|MDIA|Communication|COMM|JOUR|Literature|English|ENG|Philosophy|PHIL|Ethics|Language|World Languages|WL|Spanish|SPAN|French|FREN|German|GER|Latin|LAT|Russian|RUSN|Italian|ITAL|Chinese|CHIN|Japanese|JAPN|Classics|CLCV|Humanities|HUM|Popular Culture|POPC|Ethnic|ETHN|GSW|Women|Apparel|Merchandising|AMPD|Interior Design|ID\",\n            Major, ignore.case = TRUE) ~ \"Arts & Humanities\",\n\n      # General Studies\n      grepl(\"Liberal Studies|Individualized Studies\", Major, ignore.case = TRUE) ~ \"General Studies\",\n      grepl(\"Pre-Med|Pre-Vet|Pre-Law|Pre-Dent|Pre-Prof\", Major, ignore.case = TRUE) ~ \"Pre-Professional\",\n      grepl(\"-MIN|Certificate|Minor\", Major, ignore.case = TRUE) ~ \"Special Program (Minor/Cert)\", # Catch minors explicitly\n      is.na(Major) | Major %in% c(\"No Response\", \"\", \"Guest\", \"Non-Degree\") ~ \"Unknown/Non-Degree\",\n\n      # Default Case\n      TRUE ~ \"Other\"\n    )\n  ) %&gt;%\n    select(-c(Major_Cleaned)) # Remove the temporary cleaned column\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#visit-pattern-features",
    "href": "notebooks/report/feature_engineering.html#visit-pattern-features",
    "title": "Feature Engineering",
    "section": "Visit Pattern Features",
    "text": "Visit Pattern Features\nStudent_ID analysis enabled the construction of several usage metrics. Beyond simple visit counts (Total_Visits), we examined temporal patterns at multiple scales, calculating visits per semester (Semester_Visits) and average weekly visits (Avg_Weekly_Visits).\nadd_visit_features &lt;- function(df) {\n  df %&gt;%\n    group_by(Student_IDs) %&gt;%\n    mutate(\n      # Count visits per student\n      Total_Visits = n(),\n      # Count distinct visit dates per student per semester\n      Semester_Visits = n_distinct(Check_In_Date), # Assumes data is per semester, group_by(Student_IDs, Semester) might be safer if not\n      # Average visits per week (approximate)\n      Avg_Weekly_Visits = Semester_Visits / max(Semester_Week, na.rm = TRUE) # Use max week in semester\n    ) %&gt;%\n    ungroup()\n}\nExamination of visit frequency throughout the semester revealed clear patterns. Weeks 1-3, 9, 14, and 17 consistently showed lower activity levels (likely start/end of term, breaks), while the remaining weeks demonstrated higher traffic. We encoded this insight through a categorical Week_Volume feature.\nadd_week_volume_category &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      Week_Volume = case_when(\n        Semester_Week %in% c(4:8, 10:13, 15:16) ~ \"High Volume\",\n        Semester_Week %in% c(1:3, 9, 14, 17) ~ \"Low Volume\",\n        TRUE ~ \"Other\" # Handle potential NAs or unexpected week numbers\n      )\n    )\n}\n\n\n\nWeek Visits",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#course-load-and-performance-features",
    "href": "notebooks/report/feature_engineering.html#course-load-and-performance-features",
    "title": "Feature Engineering",
    "section": "Course Load and Performance Features",
    "text": "Course Load and Performance Features\nFor each student-semester combination, we developed metrics to capture academic context. We tracked the number of unique courses (Unique_Courses), the diversity of course levels taken (Course_Level_Mix), and the proportion of upper-division courses (Advanced_Course_Ratio).\nadd_course_load_features &lt;- function(df) {\n  df %&gt;%\n    group_by(Student_IDs, Semester) %&gt;%\n    mutate(\n      # Number of unique courses taken by student in that semester\n      Unique_Courses = n_distinct(Course_Number),\n      # Mix of course levels taken by student in that semester\n      Course_Level_Mix = n_distinct(Course_Code_by_Thousands),\n      # Proportion of advanced courses ('Upper Classmen' level) taken by student in that semester\n      Advanced_Course_Ratio = mean(Course_Level == \"Upper Classmen\", na.rm = TRUE)\n    ) %&gt;%\n    ungroup()\n}\nAdditionally, we implemented a GPA trend indicator (GPA_Trend) using the sign() function on the Change_in_GPA column. This focuses on the direction of GPA change (positive, negative, or zero) rather than the magnitude.\nadd_gpa_trend &lt;- function(df) {\n  df %&gt;% mutate(\n    # Calculate GPA trend (1 for positive, -1 for negative, 0 for no change/NA)\n    GPA_Trend = sign(Change_in_GPA),\n  )\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#group-dynamics",
    "href": "notebooks/report/feature_engineering.html#group-dynamics",
    "title": "Feature Engineering",
    "section": "Group Dynamics",
    "text": "Group Dynamics\nA final analytical step involved identifying potential group study patterns. By counting occurrences of identical Check_In_Timestamp values (combined date and time), we estimated Group_Size. This led to a boolean Group_Check_In flag and a categorical Group_Size_Category.\nadd_group_features &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      Check_In_Timestamp = ymd_hms(paste(Check_In_Date, Check_In_Time))\n    ) %&gt;%\n    # Count how many rows share the exact same check-in timestamp\n    add_count(Check_In_Timestamp, name = \"Group_Size\") %&gt;%\n    mutate(\n      # Flag if group size is greater than 1\n      Group_Check_In = Group_Size &gt; 1,\n      # Categorize group size\n      Group_Size_Category = case_when(\n        Group_Size == 1 ~ \"Individual\",\n        Group_Size &lt;= 3 ~ \"Small Group\",\n        Group_Size &lt;= 6 ~ \"Medium Group\",\n        TRUE ~ \"Large Group\" # Includes &gt; 6\n      )\n    ) %&gt;%\n    # Remove the temporary timestamp column\n    select(-Check_In_Timestamp)\n}\nWhile some simultaneous check-ins might be coincidental, this classification captures potential social patterns in Learning Commons usage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#data-quality-response-variable-handling",
    "href": "notebooks/report/feature_engineering.html#data-quality-response-variable-handling",
    "title": "Feature Engineering",
    "section": "Data Quality & Response Variable Handling",
    "text": "Data Quality & Response Variable Handling\nEssential validation and processing steps were included for the target variables (Duration_In_Min and Occupancy) and related features.\nWe ensured Duration_In_Min was calculated correctly from check-in/out times and that negative durations (data errors) were handled (set to NA and filtered).\nensure_duration &lt;- function(df) {\n  # Calculate duration in minutes\n  df %&gt;%\n    mutate(\n      Duration_In_Min = as.numeric(difftime(\n        Check_Out_Time,\n        Check_In_Time,\n        units = \"mins\"\n      )),\n      # Handle negative durations (likely data errors)\n      Duration_In_Min = if_else(Duration_In_Min &lt; 0, NA_real_, Duration_In_Min),\n    ) %&gt;%\n    # Remove rows where duration could not be calculated or was negative\n    filter(!is.na(Duration_In_Min))\n}\nWe also created a categorical version of duration, Session_Length_Category.\nadd_session_length_category &lt;- function(df) {\n  df %&gt;% mutate(\n    # Add session length categories based on Duration_In_Min\n    Session_Length_Category = case_when(\n      Duration_In_Min &lt;= 30 ~ \"Short\",\n      Duration_In_Min &lt;= 90 ~ \"Medium\",\n      Duration_In_Min &lt;= 180 ~ \"Long\",\n      Duration_In_Min &gt; 180 ~ \"Extended\",\n      TRUE ~ NA_character_ # Handle cases where Duration_In_Min might be NA\n    )\n  )\n}\nThe Occupancy variable (number of students present at check-in time) was calculated by tracking cumulative arrivals and departures within each day.\ncalculate_occupancy &lt;- function(df) {\n  df %&gt;%\n    # Ensure data is ordered chronologically within each day\n    arrange(Check_In_Date, Check_In_Time) %&gt;%\n    group_by(Check_In_Date) %&gt;%\n    mutate(\n      # Cumulative arrivals on this day up to this point\n      Cum_Arrivals = row_number(),\n      # Cumulative departures on this day before or at this check-in time\n      Cum_Departures = sapply(seq_along(Check_In_Time), function(i) {\n        sum(!is.na(Check_Out_Time[1:i]) & \n            Check_Out_Time[1:i] &lt;= Check_In_Time[i])\n      }),\n      # Occupancy is arrivals minus departures\n      Occupancy = Cum_Arrivals - Cum_Departures\n    ) %&gt;%\n    # Remove temporary cumulative columns\n    select(-c(Cum_Arrivals, Cum_Departures)) %&gt;%\n    ungroup() # Ungroup after calculation\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/feature_engineering.html#conclusion",
    "href": "notebooks/report/feature_engineering.html#conclusion",
    "title": "Feature Engineering",
    "section": "Conclusion",
    "text": "Conclusion\nOur feature engineering process addressed key challenges in the Learning Commons dataset through systematic transformation and enrichment. Temporal features capture cyclical patterns and academic calendar effects. Course-related variable treatments reduce dimensionality while preserving meaningful distinctions. The dual approach to student classification provides complementary perspectives.\nThe detailed keyword-based categorization for Course_Name, Course_Type, and Major balances granularity and practicality, enhancing interpretability despite some consolidation. Visit pattern features capture individual and facility-wide trends. The group dynamics features offer insights into collaborative usage.\nExtensive validation and calculation steps for Duration_In_Min and Occupancy ensure data quality. These engineered features form a robust foundation for modeling, though further refinement is always possible.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Feature Engineering</span>"
    ]
  },
  {
    "objectID": "notebooks/report/features_external.html",
    "href": "notebooks/report/features_external.html",
    "title": "External Data",
    "section": "",
    "text": "Data Handling\nThe complete implementation for integrating external data can be found in our source code.\nBeyond the features derived directly from the Learning Commons dataset, we incorporated external data sources to potentially capture broader environmental influences on student behavior.\nBefore adding external features, the script first combines the train_engineered.csv and test_engineered.csv files (output from the primary feature engineering script). An origin column is added to track which dataset each row came from. This allows external data fetching (like weather) to be done efficiently across the entire date range. After adding external features, the combined dataset is split back into train and test sets based on the origin column before saving the final outputs.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>External Data</span>"
    ]
  },
  {
    "objectID": "notebooks/report/features_external.html#moon-phase-features",
    "href": "notebooks/report/features_external.html#moon-phase-features",
    "title": "External Data",
    "section": "Moon Phase Features",
    "text": "Moon Phase Features\nWe (Jaryt) hypothesized (wanted to make Jason cringe) that lunar cycles might subtly influence activity patterns. To explore this, we used the R lunar package to calculate the moon phase for each Check_In_Date.\n# Relevant snippet from src/r/utils/create_data/external_data.R\nlibrary(lunar)\n\ndata_full &lt;- data_full %&gt;%\n  mutate(\n    Moon_Phase_Radians = lunar::lunar.phase(Check_In_Date, name = FALSE), # Get numeric phase in radians\n    Cosine_Phase = cos(Moon_Phase_Radians)\n    # Original code also calculated 8 discrete phases, but only Cosine_Phase was kept\n  ) %&gt;%\n  select(-Moon_Phase_Radians) # Remove the intermediate radians column\nThe lunar.phase function returns the phase as a value in radians. We then calculated the cosine of this value (Cosine_Phase). Using the cosine transforms the cyclical phase information into a continuous variable ranging from -1 to 1, which can be more easily incorporated into regression models than categorical phase names.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>External Data</span>"
    ]
  },
  {
    "objectID": "notebooks/report/features_external.html#weather-features",
    "href": "notebooks/report/features_external.html#weather-features",
    "title": "External Data",
    "section": "Weather Features",
    "text": "Weather Features\nLocal weather conditions could plausibly affect whether students choose to visit the Learning Commons or how long they stay. We used the openmeteo R package to fetch historical hourly weather data for Bowling Green, Ohio.\n\nDefine Parameters: We identified the date range from our combined dataset and specified the geographic coordinates for Bowling Green.\n# Relevant snippet from src/r/utils/create_data/external_data.R\nstart_date &lt;- min(data_full$Check_In_Date)\nend_date &lt;- max(data_full$Check_In_Date)\n\nbowling_green_coords &lt;- c(\n  latitude = 41.374775,\n  longitude = -83.651321\n)\nFetch Data: We requested various hourly metrics (including temperature, cloud cover, wind speed, precipitation, rain, snowfall, etc.) using the weather_history function. Error handling was included in case the API request failed.\nlibrary(openmeteo)\n\nresponse_units &lt;- list(\n  temperature_unit = \"fahrenheit\",\n  windspeed_unit = \"kmh\",\n  precipitation_unit = \"mm\"\n)\n\nall_hourly_vars &lt;- weather_variables()[[\"hourly_history_vars\"]]\n\nweather_history &lt;- tryCatch({\n    weather_history(\n      location = bowling_green_coords,\n      start = start_date,\n      end = end_date,\n      response_units = response_units,\n      hourly = all_hourly_vars\n    )\n}, error = function(e) {\n    warning(glue(\"Failed to fetch weather data: {e$message}\\nProceeding without weather features.\"), call. = FALSE)\n    NULL # Return NULL if fetching fails\n})\nPrepare and Merge: If the weather data was fetched successfully, we prepared it for joining. This involved selecting relevant columns, renaming them with a weather_ prefix, and creating a common Hourly_Timestamp_Floor key in both the weather data and our main dataset (by flooring the Check_In_Datetime to the beginning of the hour as preventing future weather from influencing a student’s decision to come or go to the LC). A left_join merged the corresponding hourly weather data onto each visit record.\n# Relevant snippet from src/r/utils/create_data/external_data.R\nif (!is.null(weather_history)) {\n    weather_to_join &lt;- weather_history %&gt;%\n      select(datetime, starts_with(\"hourly_\")) %&gt;%\n      rename_with(~ paste0(\"weather_\", .), starts_with(\"hourly_\")) %&gt;%\n      rename(Hourly_Timestamp_Floor = datetime)\n\n    data_full &lt;- data_full %&gt;%\n      mutate(\n        Check_In_Datetime = ymd_hms(paste(Check_In_Date, Check_In_Time), quiet = TRUE),\n        Hourly_Timestamp_Floor = floor_date(Check_In_Datetime, \"hour\")\n      )\n\n    data_full_ext &lt;- left_join(data_full, weather_to_join, by = \"Hourly_Timestamp_Floor\")\n\n    data_full_ext &lt;- data_full_ext %&gt;%\n      select(-Check_In_Datetime, -Hourly_Timestamp_Floor)\n} else {\n    # Handle case where weather fetch failed\n    data_full_ext &lt;- data_full\n}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>External Data</span>"
    ]
  },
  {
    "objectID": "notebooks/report/features_external.html#final-output",
    "href": "notebooks/report/features_external.html#final-output",
    "title": "External Data",
    "section": "Final Output",
    "text": "Final Output\nAfter potentially adding the moon phase and weather features, the combined dataset was split back into training and testing sets based on the original origin flag and saved as train_engineered.csv and test_engineered.csv in the data/processed directory, overwriting the intermediate files generated by feature_engineering.R.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>External Data</span>"
    ]
  },
  {
    "objectID": "notebooks/report/preprocessing.html",
    "href": "notebooks/report/preprocessing.html",
    "title": "Preprocessing Pipelines",
    "section": "",
    "text": "R tidymodels Preprocessing (recipes)\nConsistent and appropriate preprocessing is crucial for effective modeling. While our feature engineering (detailed previously) was performed entirely in R, the final preprocessing steps before model training were handled slightly differently within the R and Python frameworks.\nFor the models trained using the R tidymodels ecosystem (MARS, Random Forest, XGBoost), we defined a standardized preprocessing pipeline using the recipes package. This object, created by the create_recipe function in src/r/recipes/recipes.R, encapsulated the necessary steps to prepare the engineered data for these specific algorithms.\nKey steps included:\nThis approach ensured that the data fed into the tidymodels workflows was consistently processed according to the needs of the algorithms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing Pipelines</span>"
    ]
  },
  {
    "objectID": "notebooks/report/preprocessing.html#r-tidymodels-preprocessing-recipes",
    "href": "notebooks/report/preprocessing.html#r-tidymodels-preprocessing-recipes",
    "title": "Preprocessing Pipelines",
    "section": "",
    "text": "Role Definition: Assigning ‘outcome’ and ‘predictor’ roles to variables.\nVariable Removal: Removing identifier columns, raw date/time fields (after extracting relevant features like minutes past midnight), and the other target variable not being predicted in a given run.\nTime Conversion: Transforming Check_In_Time into a numeric Check_In_Time_Minutes feature.\nImputation: Handling missing numeric values using mean imputation (step_impute_mean).\nCategorical Handling: Preparing for potential new factor levels during resampling (step_novel) and converting nominal predictors to numeric using dummy variables (step_dummy with one_hot = FALSE to mimic pandas drop_first=True).\nVariance Filtering: Removing predictors with zero variance (step_zv), often necessary after dummy coding.\nNormalization: Centering and scaling all numeric predictors (step_normalize).\n\n\n# Example recipe creation (from src/r/recipes/recipes.R)\ncreate_recipe &lt;- function(data, outcome_var, features_to_drop) {\n    # ... (Input validation) ...\n    other_target &lt;- setdiff(c(\"Duration_In_Min\", \"Occupancy\"), outcome_var)\n    all_drops &lt;- unique(c(features_to_drop, other_target))\n    outcome_sym &lt;- rlang::sym(outcome_var)\n\n    rec &lt;- recipe(data) %&gt;%\n        update_role(!!outcome_sym, new_role = \"outcome\") %&gt;%\n        update_role(all_nominal(), -all_outcomes(), new_role = \"predictor\") %&gt;%\n        update_role(all_numeric(), -all_outcomes(), new_role = \"predictor\") %&gt;%\n        step_rm(dplyr::any_of(all_drops)) %&gt;%\n        step_mutate(Check_In_Time_Minutes = (lubridate::hour(lubridate::hms(Check_In_Time)) * 60 +\n            lubridate::minute(lubridate::hms(Check_In_Time)))) %&gt;%\n        step_rm(Check_In_Time) %&gt;%\n        step_rm(dplyr::any_of(c(\"Check_In_Date\", \"Semester_Date\", \"Expected_Graduation_Date\"))) %&gt;%\n        step_impute_mean(all_numeric_predictors()) %&gt;%\n        step_novel(all_nominal_predictors()) %&gt;%\n        step_dummy(all_nominal_predictors(), one_hot = FALSE) %&gt;%\n        step_zv(all_predictors()) %&gt;%\n        step_normalize(all_numeric_predictors())\n\n    return(rec)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing Pipelines</span>"
    ]
  },
  {
    "objectID": "notebooks/report/preprocessing.html#python-framework-preprocessing",
    "href": "notebooks/report/preprocessing.html#python-framework-preprocessing",
    "title": "Preprocessing Pipelines",
    "section": "Python Framework Preprocessing",
    "text": "Python Framework Preprocessing\nFor the neural network models (MLP, GRU) implemented in Python using PyTorch, the preprocessing started with the output of the R feature engineering process (typically loaded as CSV or parquet files into pandas DataFrames).\nKey Python preprocessing steps included:\n\nColumn Dropping: Removing identifier columns or other raw features not directly used by the models.\nColumn Alignment: Ensuring consistency in the feature set between training, validation, and holdout splits. Since dummy variables were created in the R recipe step, this involved adding columns with zero values if they were present in the training set but not a validation/holdout fold, and removing columns present in validation/holdout but not training.\nBoolean Conversion: Converting any remaining boolean columns (often resulting from dummy encoding) to integer format (0 or 1).\nFeature Scaling: Applying sklearn.preprocessing.StandardScaler to all numeric features. Crucially, the scaler was fit only on the training portion of the data (the 75% used for CV) and then used to transform the training, validation, and holdout sets.\n\nThese steps prepared the data specifically for ingestion by the PyTorch models and DataLoaders.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Preprocessing Pipelines</span>"
    ]
  },
  {
    "objectID": "notebooks/report/model_tuning.html",
    "href": "notebooks/report/model_tuning.html",
    "title": "Model Tuning Overview",
    "section": "",
    "text": "Framework Architecture\nThis document outlines the different modeling approaches and hyperparameter tuning strategies employed in this project. For details on the preprocessing steps applied before model training, please refer to the preprocessing_pipelines.md document.\nTo tackle the prediction tasks for visit duration and building occupancy, we adopted a pragmatic, hybrid modeling strategy. Recognizing the strengths of different toolkits, we combined the R ecosystem for some tasks and Python for others, using the engineered features from our R pipeline as common ground.\nThis blend allowed us to leverage familiar and effective tools for standard models while retaining the power to experiment with more complex neural network designs.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Tuning Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/report/model_tuning.html#framework-architecture",
    "href": "notebooks/report/model_tuning.html#framework-architecture",
    "title": "Model Tuning Overview",
    "section": "",
    "text": "R (tidymodels): This was our workhorse for setting up and tuning Multivariate Adaptive Regression Splines (MARS), Random Forest, and XGBoost models. The structured approach of tidymodels (specifically parsnip for defining models, recipes for preprocessing, and tune for optimization) streamlined the workflow for these established algorithms. Key scripts include src/r/models/models.R and scripts/run_pipeline.R.\nPython (PyTorch): For exploring neural network approaches, we turned to Python and PyTorch. This gave us the flexibility to define custom architectures for a Multi-Layer Perceptron (MLP) and a Gated Recurrent Unit (GRU) network. The relevant code can be found in src/python/tuning/gruey.py (for GRU tuning via Optuna) and notebooks/team/neuralnets.rmd (for MLP exploration).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Tuning Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/report/model_tuning.html#cross-validation-and-holdout-strategy",
    "href": "notebooks/report/model_tuning.html#cross-validation-and-holdout-strategy",
    "title": "Model Tuning Overview",
    "section": "Cross-Validation and Holdout Strategy",
    "text": "Cross-Validation and Holdout Strategy\nEnsuring our models generalize beyond the data they were trained on was paramount. We implemented a consistent validation process across both R and Python frameworks:\n\nInitial Split: We first took the LC_train dataset (containing the fully labeled and engineered data from Fall 2016 - Spring 2017) and set aside 25% as a final holdout set. This data was never used during model tuning or selection.\nCross-Validation: The remaining 75% of the LC_train data became our training/validation set. We used 5-fold cross-validation on this set to tune the hyperparameters for each model (MARS, RF, XGBoost, MLP, GRU).\nTuning: Within the 5 folds, models were trained on 4 folds and evaluated on the remaining fold. Performance metrics (primarily RMSE) averaged across the folds helped us identify the best-performing hyperparameter configuration for each algorithm.\nFinal Training: Once the best hyperparameters were chosen for a model, we retrained that model one last time using the entire 75% training/validation set.\nHoldout Evaluation: Finally, this retrained model was evaluated on the 25% holdout set we initially set aside. This provided the final, unbiased assessment of how well the model was likely to perform on truly unseen data.\n\nThis multi-step process helps guard against overfitting and gives us more confidence in our final performance metrics.\n (Note: Diagram illustrates the general concept. Our specific process used 5 folds and an initial 80/20 split for the holdout set.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Tuning Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/report/model_tuning.html#core-algorithms-tuning",
    "href": "notebooks/report/model_tuning.html#core-algorithms-tuning",
    "title": "Model Tuning Overview",
    "section": "Core Algorithms & Tuning",
    "text": "Core Algorithms & Tuning\n\nR tidymodels Framework (MARS, Random Forest, XGBoost)\nThis part of the pipeline leveraged the integrated tools within tidymodels for model specification and hyperparameter tuning.\nAlgorithms & Tuning:\n\nMultivariate Adaptive Regression Splines (MARS):\n\nConcept: Builds a model using linear segments (hinge functions) to capture non-linearities.\nSpecification (parsnip):\n# From src/r/models/models.R\nmars_spec &lt;- mars(\n  mode = \"regression\", num_terms = tune(), prod_degree = tune()\n) %&gt;%\n  set_engine(\"earth\")\nTuned Hyperparameters (dials grid):\n\nnum_terms: Number of hinge functions. Explored ranges like [7, 15] (duration) and [120, 130] (occupancy).\nprod_degree: Maximum interaction degree between terms. Fixed at 1 (no interactions).\n\n# Example grid for occupancy (from src/r/models/models.R)\nmars_grid_occ &lt;- grid_regular(\n  parameters(\n    num_terms(range = c(120L, 130L)),\n    prod_degree(range = c(1L, 1L))\n  ),\n  levels = c(num_terms = 10, prod_degree = 1)\n)\n\nRandom Forest:\n\nConcept: An ensemble of decision trees, reducing variance and improving robustness.\nSpecification (parsnip):\n# From src/r/models/models.R\nrf_spec &lt;- rand_forest(\n  mode = \"regression\", trees = tune(), min_n = tune(), mtry = tune()\n) %&gt;%\n  set_engine(\"ranger\")\nTuned Hyperparameters (dials grid):\n\ntrees: Number of trees. Ranges like [300, 325] (duration) and [250, 350] (occupancy).\nmin_n: Min data points in a node for splitting. Ranges like [15, 25] (duration) and [2, 3] (occupancy).\nmtry: Number of predictors sampled at each split. Ranges like [20, 25] (duration) and [40, 45] (occupancy).\n\n# Example grid for occupancy (from src/r/models/models.R)\nrf_grid_occ &lt;- grid_regular(\n  parameters(\n    trees(range = c(250L, 350L)),\n    min_n(range = c(2L, 3L)),\n    mtry(range = c(40L, 45L))\n  ),\n  levels = c(trees = 3, min_n = 2, mtry = 2)\n)\n\nXGBoost:\n\nConcept: Gradient boosting machine that builds trees sequentially, correcting errors of prior trees.\nSpecification (parsnip):\n# From src/r/models/models.R\nxgb_spec &lt;- boost_tree(\n  mode = \"regression\", trees = tune(), tree_depth = tune(), learn_rate = tune(),\n  min_n = tune(), mtry = tune()\n) %&gt;%\n  set_engine(\"xgboost\")\nTuned Hyperparameters (dials grid):\n\ntrees: Number of boosting rounds. Ranges like [75, 100] (duration) and [350, 450] (occupancy).\ntree_depth: Max depth per tree. Ranges like [15, 21] (duration) and [6, 8] (occupancy).\nlearn_rate: Learning rate. Fixed values explored (e.g., 0.05, 0.1).\nmin_n: Min data points in a node. Ranges like [10, 15] (duration) and [2, 3] (occupancy).\nmtry: Predictors sampled per tree. Ranges like [12, 15] (duration) and [30, 35] (occupancy).\n\n# Example grid for occupancy (from src/r/models/models.R)\nxgb_grid_occ &lt;- grid_regular(\n  parameters(\n    trees(range = c(350L, 450L)),\n    tree_depth(range = c(6L, 8L)),\n    learn_rate(range = log10(c(0.1, 0.1))),\n    min_n(range = c(2L, 3L)),\n    mtry(range = c(30L, 35L))\n  ),\n  levels = c(trees = 3, tree_depth = 3, learn_rate = 1, min_n = 2, mtry = 2)\n)\n\n\n\n\nPython Framework (MLP, GRU)\nFor neural networks, we shifted to Python and PyTorch, performing preprocessing steps as outlined in the previous section, Preproccessing Pipelines.\nAlgorithms & Tuning:\n\nMulti-Layer Perceptron (MLP):\n\nConcept: A standard feedforward neural network.\nArchitecture Definition (nn.Module):\n# Simplified from notebooks/team/neuralnets.rmd\nimport torch\nimport torch.nn as nn\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, activation='ReLU', dropout=0.0):\n        super(SimpleNN, self).__init__()\n        # ... (logic to build sequential layers based on hidden_dims, activation, dropout)\n        layers = []\n        prev_dim = input_dim\n        # ... loop to add nn.Linear, activation, nn.Dropout ...\n        layers.append(nn.Linear(prev_dim, output_dim))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\nTuned Hyperparameters (Example Search Space):\n# Example from notebooks/team/neuralnets.rmd\nsearch_space = {\n    \"n_layers\": [1, 2, 3],\n    \"n_units_l0\": [12, 50, 100, 150], # Units in first hidden layer\n    \"n_units_l1\": [12, 50, 100],      # Units in second (if n_layers &gt;= 2)\n    \"n_units_l2\": [12, 50],          # Units in third (if n_layers == 3)\n    \"activation\": [\"ReLU\"],           # Activation function\n    \"learning_rate\": [0.01],           # Optimizer learning rate\n    \"batch_size\": [2048],             # Training batch size\n    \"dropout\": [0, 0.2, 0.3],          # Dropout rate\n    \"weight_decay\": [0, 1e-5, 1e-4]   # L2 regularization\n}\n\nGated Recurrent Unit (GRU) Network:\n\nConcept: A type of recurrent neural network suitable for sequence data, though applied here to tabular features, potentially capturing interactions differently than MLPs.\nArchitecture Definition (nn.Module): A custom GrueyModel class was defined (in src/python/models/gruey_architecture.py), incorporating GRU layers potentially followed by linear layers.\nTuned Hyperparameters (Optuna Search Space): Tuning was performed using Optuna (src/python/tuning/gruey.py).\n# Snippet from objective function in src/python/tuning/gruey.py\ndef objective(trial: optuna.trial.Trial, ...):\n    # Tunable\n    lr = trial.suggest_float(\"lr\", 1e-3, 5e-3, log=True)\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.25, 0.42)\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-5, log=True)\n    gru_dim = trial.suggest_categorical(\"gru_dim\", [128, 256, 512])\n    num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128])\n    gru_expansion = trial.suggest_float(\"gru_expansion\", 0.5, 1.4)\n    # Fixed\n    activation_fn_name = \"relu\"\n    # ... rest of objective function ...\n    return best_val_loss",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Tuning Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/report/model_tuning.html#framework-integration",
    "href": "notebooks/report/model_tuning.html#framework-integration",
    "title": "Model Tuning Overview",
    "section": "Framework Integration",
    "text": "Framework Integration\nThis combined R and Python approach allowed us to cast a wide net, evaluating traditional statistical models alongside more contemporary neural networks. The tidymodels framework offered efficient tuning for the former, while PyTorch provided the flexibility needed for the latter. Tracking experiments across both environments was facilitated by MLflow, allowing us to compare results and ultimately select the best overall models based on their performance on the holdout set, as detailed in the Evaluation section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Tuning Overview</span>"
    ]
  },
  {
    "objectID": "notebooks/report/weighted_mods.html",
    "href": "notebooks/report/weighted_mods.html",
    "title": "Weighted Duration",
    "section": "",
    "text": "Rationale for Weighting\nThis document focuses on a specific post-processing step applied to the duration predictions: weighting the model’s output with a student’s historical average visit duration. This strategy was explored under the hypothesis that individual students exhibit habitual patterns in their Learning Commons usage. Initial model fitting and hyperparameter tuning details for MARS, Random Forest, and XGBoost can be found in the model_tuning.md document.\nWe believe that students may repeatedly visit the Learning Commons and stay roughly the same amount of time each visit as it becomes part of their routine habits. To reflect this, we considered adjusting the raw model predictions by incorporating the mean duration of a student’s previous visits observed in the training data.\nUsing the historical mean visit duration directly as a predictor during model training is problematic, as it requires multiple data points per student and would complicate standard cross-validation procedures (data leakage). Therefore, we applied this adjustment after the initial model predictions were generated.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted Duration</span>"
    ]
  },
  {
    "objectID": "notebooks/report/weighted_mods.html#initial-exploration-with-fixed-weights",
    "href": "notebooks/report/weighted_mods.html#initial-exploration-with-fixed-weights",
    "title": "Weighted Duration",
    "section": "Initial Exploration with Fixed Weights",
    "text": "Initial Exploration with Fixed Weights\nWe first tested this weighting idea on the holdout set predictions from the MARS, Random Forest, and XGBoost models, initially using an arbitrary weight of w1 = 2/3 for the model prediction and w2 = 1/3 for the historical average. If a student had no prior visits in the training set, their original model prediction was used.\n# Example using MARS predictions (hfit1) and holdout data\n# Assumes 'studentsfull', 'trainfull', 'holdoutfull', 'holdout', 'hfit1' are loaded/defined\n# (Code for loading/splitting/initial MARS fit is omitted for brevity)\n\nw1 &lt;- 2/3\nw2 &lt;- 1-w1\n\nID1 &lt;- data.frame(ids = holdoutfull$Student_IDs, y = hfit1) # Using 'y' for generic prediction\n\nID1$OtherVisits &lt;- NA # Initialize column\nfor (i in 1:nrow(ID1)) {\n  mask &lt;- trainfull$Student_IDs == ID1$ids[i]\n  # Calculate mean duration from training visits ONLY\n  mean_duration &lt;- mean(trainfull$Duration_In_Min[mask], na.rm = TRUE)\n  # Only assign if not NaN (i.e., student had prior visits)\n  if (!is.nan(mean_duration)) {\n      ID1$OtherVisits[i] &lt;- mean_duration\n  }\n}\n\n# Apply weighting: use prediction if no prior visits (OtherVisits is NA), otherwise use weighted average\nID1$Weighted &lt;- ifelse(is.na(ID1$OtherVisits), ID1$y,\n                      w1*ID1$y + w2*ID1$OtherVisits)\n\n# Calculate RMSE for the weighted predictions\ntoterrorW1 &lt;- sum((ID1$Weighted - holdout$Duration_In_Min)^2)/length(ID1$Weighted)\nsqrt(toterrorW1)\n## [1] 57.78979\nApplying this fixed 2/3 weight yielded slight improvements in RMSE for all three models compared to their unweighted predictions. For example, MARS RMSE improved from ~59.23 to ~57.79, and Random Forest improved from ~57.04 to ~56.57. This suggested the weighting approach had merit.\nThis weighting approach highlights the potential relevance of individual student behavior patterns in modeling visit duration.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted Duration</span>"
    ]
  },
  {
    "objectID": "notebooks/report/weighted_mods.html#optimizing-weights-via-cross-validation-for-random-forest",
    "href": "notebooks/report/weighted_mods.html#optimizing-weights-via-cross-validation-for-random-forest",
    "title": "Weighted Duration",
    "section": "Optimizing Weights via Cross-Validation for Random Forest",
    "text": "Optimizing Weights via Cross-Validation for Random Forest\nSince Random Forest showed strong performance, we proceeded to find the optimal weight specifically for this model using a more rigorous cross-validation approach on the entire training dataset (not just the initial holdout split).\nThe final tuned hyperparameters for the Random Forest model used in this CV process were: - mtry = 20 (number of predictors sampled at each split) - trees = 200 (number of trees in the forest) - min_n = 10 (minimum number of observations in a node to allow a split)\n# --- Setup for Weight CV ---\n# Assumes 'maintrain' (full preprocessed training data) and\n# 'studentsfull' (original training data with IDs) are loaded.\n# library(caret), library(doParallel), library(here) assumed loaded.\n\n# Prepare parallel processing\nnum_cores &lt;- detectCores(logical = FALSE)\nnum_cores_to_use &lt;- max(1, num_cores - 4) # Use at least 1 core\ncl &lt;- makePSOCKcluster(num_cores_to_use)\nregisterDoParallel(cl)\n\n# Create 5 CV folds from the full training data\nset.seed(7560)\ntestInd &lt;- createFolds(maintrain$Duration_In_Min, k = 5)\n\nIDLIST &lt;- list() # To store predictions from each fold\n\n# Fixed hyperparameters for the RF model\ntune &lt;- data.frame(mtry = 20)\nnodesize_val &lt;- 10\nntree_val &lt;- 200\n\n# --- Cross-Validation Loop ---\ncat(\"Starting 5-fold CV for Random Forest predictions...\n\")\nfor (i in 1:5) {\n  cat(glue::glue(\"Processing Fold {i}/5...\n\"))\n  # Train RF on 4 folds\n  # Note: Using caret::train here for consistency with original script,\n  # but tidymodels::fit_resamples could also be used.\n  fullF &lt;- train(Duration_In_Min ~ .,\n                 data = maintrain[-testInd[[i]],], # Training data for this fold\n                 method = \"rf\",\n                 tuneGrid = tune,       # Fixed mtry\n                 ntree = ntree_val,     # Fixed ntree\n                 nodesize = nodesize_val, # Fixed nodesize (min_n)\n                 trControl = trainControl(method = \"none\")) # No inner tuning needed\n\n  # Predict on the held-out fold\n  fittedF &lt;- predict(fullF, newdata = maintrain[testInd[[i]],]) # Test data for this fold\n\n  # Store results: Fold number, Student ID, Prediction, True Value\n  IDLIST[[i]] &lt;- data.frame(Fold = i,\n                            ids = studentsfull$Student_IDs[testInd[[i]]], # Get IDs from original data\n                            fittedF = fittedF,\n                            TRUEy = maintrain$Duration_In_Min[testInd[[i]]]) # Get true values\n}\ncat(\"CV finished.\n\")\nstopCluster(cl) # Stop parallel backend\n\n# --- Calculate Historical Averages ---\n# Combine predictions from all folds\nID &lt;- do.call(rbind, IDLIST)\ncolnames(ID) &lt;- c(\"Fold\", \"ids\", \"fittedF\", \"TRUEy\")\n\ncat(\"Calculating historical average durations across folds...\n\")\nID$AveOtherVisits &lt;- NA # Initialize column\nfor (i in 1:nrow(ID)) {\n  current_id &lt;- ID$ids[i]\n  current_fold &lt;- ID$Fold[i]\n\n  # Calculate average TRUE duration for this student from OTHER folds\n  # This prevents data leakage within the CV process\n  mask &lt;- (ID$ids == current_id) & (ID$Fold != current_fold)\n  if (any(mask)) { # Check if the student exists in other folds\n      mean_duration &lt;- mean(ID$TRUEy[mask], na.rm = TRUE)\n      if (!is.nan(mean_duration)) {\n        ID$AveOtherVisits[i] &lt;- mean_duration\n      }\n  }\n  if (i %% 1000 == 0) {cat(glue::glue(\"Processed {i}/{nrow(ID)} students...\n\"))}\n}\ncat(\"Historical averages calculated.\n\")\n\n# --- Test Different Weights ---\n# Define a grid of weights (w1 for model prediction, w2=1-w1 for historical avg)\nweights_w1 &lt;- c(0, 1/6, 1/4, 1/3, 1/2, 2/3, 3/4, 5/6, 1)\nRMSE_results &lt;- c() # To store RMSE for each weight\n\ncat(\"Evaluating different weights...\n\")\nfor (j in 1:length(weights_w1)) {\n  w1 &lt;- weights_w1[j]\n  w2 &lt;- 1 - w1\n\n  # Apply the current weight combination\n  ID$Weighted &lt;- ifelse(is.na(ID$AveOtherVisits), ID$fittedF,\n                        w1 * ID$fittedF + w2 * ID$AveOtherVisits)\n\n  # Calculate RMSE across all folds for this weight combination\n  # Note: Original script calculated MSE per fold then averaged.\n  # Calculating overall RMSE directly is simpler here.\n  overall_rmse &lt;- sqrt(mean((ID$Weighted - ID$TRUEy)^2))\n  RMSE_results[j] &lt;- overall_rmse\n\n  cat(glue::glue(\"Weight w1={round(w1, 3)} -&gt; Overall CV RMSE = {round(overall_rmse, 3)}\n\"))\n}\n\n# Display results\nweight_performance &lt;- data.frame(Weight_for_Model_Pred = weights_w1, CV_RMSE = RMSE_results)\nprint(weight_performance)\nbest_weight_idx &lt;- which.min(RMSE_results)\nbest_w1 &lt;- weights_w1[best_weight_idx]\ncat(glue::glue(\"\nOptimal weight (w1) for model prediction: {round(best_w1, 3)}\n\"))\n##     weights     RMSE\n## 1 0.0000000 62.18344\n## 2 0.1666667 60.53198\n## 3 0.2500000 59.85238\n## 4 0.3333333 59.27482\n## 5 0.5000000 58.43742\n## 6 0.6666667 58.03793\n## 7 0.7500000 58.00562\n## 8 0.8333333 58.08540\n## 9 1.0000000 58.57874\nThe cross-validation results indicated that a weight w1 = 3/4 for the model prediction and w2 = 1/4 for the student’s historical average visit duration yielded the lowest overall RMSE.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted Duration</span>"
    ]
  },
  {
    "objectID": "notebooks/report/weighted_mods.html#final-prediction-generation",
    "href": "notebooks/report/weighted_mods.html#final-prediction-generation",
    "title": "Weighted Duration",
    "section": "Final Prediction Generation",
    "text": "Final Prediction Generation\nUsing the optimal weights (w1 = 3/4, w2 = 1/4) determined through cross-validation, we generated the final adjusted predictions for the original test set (or potentially new data).\n# --- Apply Optimal Weight to Final Predictions ---\n# Assumes 'modpreds' (raw RF predictions on test set) and\n# 'fulltest' (original test set data with IDs) are loaded.\n# Assumes 'ID' (CV results data frame) is available for historical averages.\n\n# Load raw predictions (replace path if needed)\nmodpreds &lt;- read.csv(here::here(\"data/predictions/Duration_In_Min_RandomForest_20250423180825_pred.csv\"))\n# Load test set features (replace path if needed)\nfulltest &lt;- read.csv(here::here(\"data/processed/test_engineered.csv\"))\n\n# Combine predictions with student IDs from the test set\npredIDs &lt;- data.frame(raw_pred = modpreds$.pred, ids = fulltest$Student_IDs) # Use .pred column\n\n# --- Lookup Historical Averages from Training CV ---\n# Calculate the overall historical average for each student from the CV results\n# Group by student ID and calculate the mean of their TRUE durations across all folds they appeared in\nhistorical_averages &lt;- aggregate(TRUEy ~ ids, data = ID, FUN = mean, na.rm = TRUE)\ncolnames(historical_averages) &lt;- c(\"ids\", \"historical_avg_duration\")\n\n# Merge these averages into the test set predictions\npredIDs &lt;- merge(predIDs, historical_averages, by = \"ids\", all.x = TRUE) # Left join\n\n# --- Apply Optimal Weights ---\nw1 &lt;- 3/4\nw2 &lt;- 1/4\n\n# Apply weighting: use raw prediction if no historical average, otherwise use weighted average\npredIDs$FINAL_prediction &lt;- ifelse(is.na(predIDs$historical_avg_duration),\n                                   predIDs$raw_pred,\n                                   w1 * predIDs$raw_pred + w2 * predIDs$historical_avg_duration)\n\n# --- Save Final Predictions ---\n# Select only the final weighted predictions\nfinal_output &lt;- data.frame(Prediction = predIDs$FINAL_prediction)\n\n# Save the final predictions (adjust path as needed)\noutput_path &lt;- here::here(\"data/predictions/Duration_FINAL_weighted.csv\")\nwrite.csv(final_output, output_path, row.names = FALSE)\n\ncat(glue::glue(\"Final weighted predictions saved to: {output_path}\n\"))\nThis weighted approach provided a modest but consistent improvement over the raw Random Forest predictions for duration, acknowledging the influence of individual student habits.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Weighted Duration</span>"
    ]
  },
  {
    "objectID": "notebooks/report/neural_networks.html",
    "href": "notebooks/report/neural_networks.html",
    "title": "Neural Networks",
    "section": "",
    "text": "Model Definition\nIn addition to the XGBoost, MARS, and Random Forest models described in the previous sections, we also wanted to explore how neural networks would perform on our two tasks.\nIn this section, we will construct a simple feedforward neural network in Python using the PyTorch library. As there are endless possible combinations of architectures paired with different hyperparameters, we will use 5 Fold cross validation to train/validate a subset of possible models. We also sampled a set of holdout test data to evaluate the overall performances of each model on data not seen during training.\nTo avoid copying an overwhelming amount of python code in our report, we’ve only included simplified code chunks to showcase our model training and evaluation processes.\nNote: In this section, we will not incorporate the post-hoc visit weighting scheme we implemented in the previous sections.\nThis code defines a class for a feedforward neural network model that consists of an input layer, one or more hiddnen layers, and an output layer. The number of and size of hidden layers, as well as activation functions and other hyperparameters, are all configurable.\nThis setup allows us to customly define a search space of hyperparameters and architectures to loop through and test in our pipeline. I also included options for dropout layers and weight decay for regularization. Since the model is so flexible and complex, these regularization options could help prevent overfitting.\nThe dropout layer randomly sets a fraction of the input units to 0 at each update during training, which helps prevent each neuron from becoming too reliant on any one input. The weight decay parameter is used to apply L2 regularization to the weights of the model, which helps prevent overfiting by penalizing large weights.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "notebooks/report/neural_networks.html#model-definition",
    "href": "notebooks/report/neural_networks.html#model-definition",
    "title": "Neural Networks",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, hidden_dims, output_dim, activation='ReLU', dropout=0.0):\n        super(SimpleNN, self).__init__()\n        if isinstance(hidden_dims, int):\n            hidden_dims = [hidden_dims]\n        if isinstance(activation, str):\n            activations = [activation] * len(hidden_dims)\n        else:\n            activations = activation\n        layers = []\n        prev_dim = input_dim\n        for h_dim, act in zip(hidden_dims, activations):\n            layers.append(nn.Linear(prev_dim, h_dim))\n            act_layer = getattr(nn, act)() if hasattr(nn, act) else nn.ReLU()\n            layers.append(act_layer)\n            if dropout &gt; 0.0:\n                layers.append(nn.Dropout(dropout))\n            prev_dim = h_dim\n        layers.append(nn.Linear(prev_dim, output_dim))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "notebooks/report/neural_networks.html#cross-validation-loop",
    "href": "notebooks/report/neural_networks.html#cross-validation-loop",
    "title": "Neural Networks",
    "section": "Cross-Validation Loop",
    "text": "Cross-Validation Loop\nLooping through each of the hyperparameters and architectures is computationally expensive, so I decided to convert our training data into a .parquet file for faster loading times. This significantly sped up the training process.\nIn this code, we can also easily define the target as Duration_In_Min or Occupancy depending on which model we want to train. In a separate code chunk, I can change the name of the saved output file to reflect the target being trained/tested.\nI also saved a houldout_indices.npy file for consistent holdout sampling and fair comparisons between models in the end.\nIn the code below, the search_space variable is a dictionary that defines all the hyperparameters and their possible values to be tested.\ndef main():\n    df = load_data('data/processed/train_engineered.parquet')\n    BASE_FEATURES_TO_DROP = [\n        'Student_IDs', 'Semester', 'Class_Standing', 'Major',\n        'Expected_Graduation', 'Course_Name', 'Course_Number',\n        'Course_Type', 'Course_Code_by_Thousands', 'Check_Out_Time',\n        'Session_Length_Category', 'Check_In_Date', 'Semester_Date',\n        'Expected_Graduation_Date',\n        'Duration_In_Min', 'Occupancy'\n    ]\n\n    target = 'Occupancy'\n\n    drop_cols = list(set(BASE_FEATURES_TO_DROP) - {target})\n    X, y = preprocess_data(df, target, drop_cols)\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    holdout_indices_path = 'data/processed/holdout_indices.npy'\n    X_non_holdout, y_non_holdout, X_holdout, y_holdout = get_holdout_split(X_scaled, y, holdout_indices_path)\n\n    search_space = {\n        \"n_layers\": [1, 2, 3],\n        \"n_units_l0\": [12, 50, 100, 150],\n        \"n_units_l1\": [12, 50, 100],\n        \"n_units_l2\": [12, 50],\n        \"activation\": [\"ReLU\"],\n        \"learning_rate\": [0.01],\n        \"batch_size\": [2048],\n        \"dropout\": [0, 0.2, 0.3],\n        \"weight_decay\": [0, 1e-5, 1e-4]\n    }\n\n    # ... Loop through all combinations of hyperparameters ...\n\n    # ... Printing and saving results ...",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "notebooks/report/neural_networks.html#holdout-evaluation",
    "href": "notebooks/report/neural_networks.html#holdout-evaluation",
    "title": "Neural Networks",
    "section": "Holdout Evaluation",
    "text": "Holdout Evaluation\nAfter each of the model configurations were tested using 5 Fold cross-validation, each combination was also fit to all non-holdout data and used to predict the holdout data. Although we might usually only refit and test the best-performing model from cross-validation, I suspected the larger architectures would dominate during this process and risk overfitting. Thus, I included all the models so the simpler architectures would also have a chance. The results were saved to a .csv file for easy comparison and analysis.\nThe following code is a simplified version of the retraining and evaluation process Where RMSE, MAE, and R2 metrics are calculated.\ndef retrain_and_evaluate_on_holdout(best_params, X_non_holdout, y_non_holdout, X_holdout, y_holdout):\n\n    # ... verbose for debugging ...\n    # ... refit each model on the non-holdout data ...\n\n    model.eval()\n    preds, targets_list = [], []\n    with torch.no_grad():\n        for features, targets in val_loader:\n            features, targets = features.to(device), targets.to(device)\n            outputs = model(features)\n            preds.append(outputs.cpu().numpy())\n            targets_list.append(targets.cpu().numpy())\n    preds_concat = np.concatenate(preds, axis=0)\n    targets_concat = np.concatenate(targets_list, axis=0)\n    rmse = np.sqrt(mean_squared_error(targets_concat, preds_concat))\n    mae = np.mean(np.abs(targets_concat - preds_concat))\n    ss_res = np.sum((targets_concat - preds_concat) ** 2)\n    ss_tot = np.sum((targets_concat - np.mean(targets_concat)) ** 2)\n    r2 = 1 - ss_res / ss_tot if ss_tot &gt; 0 else float('nan')\n    print(f\"Holdout Results: RMSE={rmse:.4f} | MAE={mae:.4f} | R2={r2:.4f}\")\n    return rmse, mae, r2",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "notebooks/report/neural_networks.html#results",
    "href": "notebooks/report/neural_networks.html#results",
    "title": "Neural Networks",
    "section": "Results",
    "text": "Results\nThe results of both the cross validation and holdout evaluations were surprisingly underwhelming. None of the model configurations seemed perform better than our previous XGBoost, MARS, or Random Forest models. So, they will not be used for our final predictions, but we will include two of the model configurations as part of our required 5 models for the project.\nArchitectures, hyperparameters, and resulting RMSE scores can be seen in the table below.\n\nDuration Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Name\n# Layers\nHidden Units\nActivation(s)\nDropout\nWeight Decay\nCV RMSE\nHoldout RMSE\n\n\n\n\nNeuralNet-1\n3\n[100 &gt; 50 &gt; 50]\nReLU\n0.3\n1e-4\n59.67\n62.02\n\n\nNeuralNet-2\n3\n[150 &gt; 50 &gt; 50]\nReLU\n0.3\n1e-4\n59.59\n62.05\n\n\n\n\n\nOccupancy Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Name\n# Layers\nHidden Units\nActivation(s)\nDropout\nWeight Decay\nCV RMSE\nHoldout RMSE\n\n\n\n\nNeuralNet-1\n1\n[150]\nReLU\n0.2\n1e-5\n3.62\n3.17\n\n\nNeuralNet-2\n1\n[100]\nReLU\n0.2\n1e-4\n3.67\n3.23",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "notebooks/report/eval.html",
    "href": "notebooks/report/eval.html",
    "title": "Evaluation",
    "section": "",
    "text": "Best Model Configurations\nThis section details the performance of our final selected models on the holdout dataset (the 25% of LC_train reserved before cross-validation).\nAfter tuning MARS, Random Forest, XGBoost, MLP, and GRU models using 5-fold cross-validation, XGBoost emerged as the top performer for both prediction tasks.\nFor predicting visit Duration, the optimized XGBoost model achieved the lowest Root Mean Squared Error (RMSE) on the holdout set compared to the other algorithms tested. The key hyperparameters identified through cross-validation tuning were 75 trees, a maximum tree depth of 21, a learning rate of 0.05, a minimum node size of 15, and sampling 15 variables at each split.\nHowever, the resulting R² value of 0.099 is quite low, indicating that even the best model struggled significantly to explain the variance in individual visit durations. As noted in the presentation, we found that applying a post-hoc weighted average (blending the model’s prediction with the overall mean training duration) slightly improved practical performance, though the fundamental challenge of predicting this noisy target remains.\nThe Occupancy prediction task yielded much more successful results. Again, XGBoost provided the best performance on the holdout data. The optimal configuration used 450 trees, a maximum depth of 8, a learning rate of 0.1, a minimum node size of 2, and sampled 35 variables per split. This model achieved an RMSE of 1.83 students and a strong R² value of 0.911, demonstrating its ability to effectively capture the patterns driving building occupancy based on check-in data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "notebooks/report/eval.html#best-model-configurations",
    "href": "notebooks/report/eval.html#best-model-configurations",
    "title": "Evaluation",
    "section": "",
    "text": "Duration Model (Best on Holdout)\n\n\n\nComponent\nConfiguration\n\n\n\n\nModel\nXGBoost\n\n\nCV Method\n5-fold\n\n\nTrees\n75\n\n\nTree Depth\n21\n\n\nLearning Rate\n0.05\n\n\nMin Node Size\n15\n\n\nVariables Tried (mtry)\n15\n\n\nHoldout RMSE\n59.9\n\n\nHoldout R²\n0.099\n\n\n\n\n\n\n\nOccupancy Model (Best on Holdout)\n\n\n\nComponent\nConfiguration\n\n\n\n\nModel\nXGBoost\n\n\nCV Method\n5-fold\n\n\nTrees\n450\n\n\nTree Depth\n8\n\n\nLearning Rate\n0.1\n\n\nMin Node Size\n2\n\n\nVariables Tried (mtry)\n35\n\n\nHoldout RMSE\n1.83\n\n\nHoldout R²\n0.911",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "notebooks/report/eval.html#model-diagnostics",
    "href": "notebooks/report/eval.html#model-diagnostics",
    "title": "Evaluation",
    "section": "Model Diagnostics",
    "text": "Model Diagnostics\n(Note: The diagnostic plots shown here are from the final presentation and reflect the performance of the best XGBoost models on the holdout set.)\n\nDuration Model Performance (XGBoost)\nThe diagnostic plots for the best duration model (XGBoost) highlight the difficulties encountered. While better than simpler models explored earlier, the R² of ~0.1 signifies substantial unexplained variance.\n\n\n\nDuration Model Diagnostics\n\n\n\nPredicted vs. Actual: Shows significant scatter and a tendency for the model to predict within a narrower range than the true values, particularly struggling with very long durations.\nResiduals vs. Predicted: Ideally shows no pattern, but here we see variance increase with predicted value (heteroscedasticity) especially from around 45 to 120 in our predicted values, indicating model misspecification.\nResidual Histogram/QQ Plot: These plots deviate from normality, especially in the tails, reflecting the model’s difficulty with the right skewed nature of duration data and extreme values.\n\n\nSummary: Predicting individual visit duration remains challenging. The XGBoost model, while the best performer among those tested, still only explains about 10% of the variance (R² ≈ 0.1). The diagnostics show limitations in capturing the full range and variability of visit lengths, potentially due to inherent randomness in individual behavior or missing predictive factors (e.g., specific purpose of visit).\n\n\n\nOccupancy Model Performance (XGBoost)\nThe Occupancy model diagnostics show a much better fit, consistent with the high R² value.\n\n\n\nOccupancy Model Diagnostics\n\n\n\nPredicted vs. Actual: Shows points clustering much more closely around the ideal 45-degree line, indicating good agreement between predictions and actual occupancy counts. The integer nature of the data is visible.\nResiduals vs. Predicted: While perhaps not perfectly random, the pattern is much less pronounced than for the duration model, suggesting the model captures the primary drivers of occupancy well.\nResidual Histogram/QQ Plot: Appear much closer to a normal distribution compared to the duration residuals, although some significant deviations occur at the extremes (very low and very high occupancy).\n\n\nSummary: The XGBoost model for occupancy prediction performed remarkably well (R² ≈ 0.91). Diagnostics confirm a strong fit, even for high values. The model effectively learned the relationship between the input features (time, student demographics, academic context, etc.) and the number of students currently in the Learning Commons.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "notebooks/report/eval.html#distribution-analysis",
    "href": "notebooks/report/eval.html#distribution-analysis",
    "title": "Evaluation",
    "section": "Distribution Analysis",
    "text": "Distribution Analysis\n(Note: The distribution plots shown here are from the final presentation.)\n\nPredicted vs. Actual Distributions\nComparing the distributions of predicted versus actual values provides further insight.\n\n\n\nDistribution Comparisons Duration\n\n\n\nDuration: The predicted distribution (blue) for the XGBoost duration model is still more concentrated than the actual distribution (red), reflecting the difficulty in predicting the long tail of very long visits (R² ≈ 0.1). While potentially better than simpler models from last semester, it doesn’t fully capture the observed spread.\n\n\n\n\nDistribution Comparisons Occupancy\n\n\n\nOccupancy: The predicted distribution aligns interestingly with the actual occupancy counts. While the model performs with a high R² (≈ 0.91), the shape and central tendency of the observed data sits some 2 occupants lower than the mean and median at a much higher density, \\(\\sim 2 \\times\\). This contrasts to our last semester’s prediction curve which centered about 1.5 mean occupants higher than the training distribution.\n\n\n\nComparison of Prediction Methods (Occupancy)\n(Note: The presentation included a plot comparing imputed vs. direct occupancy predictions.)\nThe visualization below compares direct occupancy prediction (blue), using XGBoost, against an indirect method (red), viz. imputing occupancy based on the XGBoost duration predictions. Direct modeling typically yields better results as it avoids propagating errors from an intermediate prediction step. However, based on the plot, one might think that the imputed curve (red) better aligns with the training occupancy distribution. This observation was noted last semester as well.\n\n\n\nOccupancy Comparison Histogram\n\n\n\nInsight: Intuitively, directly modeling the target variable of interest (Occupancy) should prove much more effective than trying to infer it from predictions of a related but harder-to-predict variable (Duration). We still wonder whether our final XGBoost occupancy model would significantly outperformed any imputed approaches considered.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Evaluation</span>"
    ]
  },
  {
    "objectID": "notebooks/report/predictions.html",
    "href": "notebooks/report/predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "Final Model Training (train_final_model.R)\nThis chapter describes the process used to train the final models on the complete training dataset and generate predictions for the unlabeled test dataset (LC_test). This workflow leverages the best model configurations identified during the cross-validation and evaluation phases (detailed in evaluation.md).\nThe process is managed by two primary R scripts:\nThis script ensures the final production model utilizes all available labeled data for maximum performance.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "notebooks/report/predictions.html#final-model-training-train_final_model.r",
    "href": "notebooks/report/predictions.html#final-model-training-train_final_model.r",
    "title": "Predictions",
    "section": "",
    "text": "Identify Best Configuration: The script first searches the model artifacts directory (artifacts/models/r/) to find the files corresponding to the best performing model for the specified TARGET_VARIABLE (e.g., “Occupancy”) based on the primary tuning metric (TUNING_METRIC, e.g., “rmse”). It extracts the model type (e.g., “XGBoost”) and loads the best hyperparameter set (_best_params.rds).\n# Conceptual logic from train_final_model.R\nbest_model_files &lt;- find_best_model_files(TARGET_VARIABLE, TUNING_METRIC)\nbest_params &lt;- readRDS(best_model_files$params_file)\nmodel_type &lt;- best_model_files$model_type\nLoad Full Training Data: It loads the complete train_engineered.csv dataset.\n# Conceptual logic from train_final_model.R\nfull_data &lt;- load_data(DATA_FILENAME) # Loads LC_train\nRebuild Workflow: The script rebuilds the tidymodels workflow:\n\nCreates the preprocessing recipe using the full training data (create_recipe from src/r/recipes/recipes.R).\nGets the appropriate parsnip model specification for the best model_type (e.g., xgb_spec from src/r/models/models.R).\nCombines the recipe and model spec into a workflow object.\nFinalizes the workflow using the loaded best_params.\n\n# Conceptual logic from train_final_model.R\nrecipe_obj &lt;- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)\nmodel_spec &lt;- model_list_to_use[[model_type]]$spec # Get spec based on type\nworkflow_obj &lt;- build_workflow(recipe_obj, model_spec)\nfinal_workflow &lt;- finalize_workflow(workflow_obj, best_params)\nFit Final Model: The finalized workflow is trained (fit()) using the entire full_data.\n# Conceptual logic from train_final_model.R\nfinal_fit &lt;- fit(final_workflow, data = full_data)\nSave Model: The final fitted workflow object (final_fit) is saved to the artifacts directory with a timestamp and model type identifier. A copy is also saved with a _latest.rds suffix for easy access by the prediction script.\n# Conceptual logic from train_final_model.R\nfinal_model_path &lt;- file.path(MODEL_DIR, final_model_filename)\nsaveRDS(final_fit, final_model_path)\n# ... code to create/update _latest.rds symlink/copy ...",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "notebooks/report/predictions.html#prediction-generation-predict_on_test.r",
    "href": "notebooks/report/predictions.html#prediction-generation-predict_on_test.r",
    "title": "Predictions",
    "section": "Prediction Generation (predict_on_test.R)",
    "text": "Prediction Generation (predict_on_test.R)\nThis script handles the application of the trained model to new, unlabeled data.\n\nLoad Final Model: It locates and loads the appropriate final fitted model object (preferring the _latest.rds version) for the specified TARGET_PREDICTION.\n# Conceptual logic from predict_on_test.R\n# ... logic to find model_path for _latest.rds or timestamped .rds ...\nbest_model_object &lt;- readRDS(model_path)\nLoad Test Data: The script loads the test_engineered.csv dataset, which contains the features for the prediction period (Fall 2017-Spring 2018) but no target variable values.\n# Conceptual logic from predict_on_test.R\ntest_data &lt;- readr::read_csv(TEST_DATA_PATH, show_col_types = FALSE)\nGenerate Predictions: The predict() function is called on the loaded model object with the test_data. The tidymodels workflow automatically applies the same preprocessing steps (defined in the recipe embedded within the fitted workflow) to the test data before feeding it to the underlying model engine (e.g., XGBoost).\n# Conceptual logic from predict_on_test.R\n# Assumes make_predictions uses predict() internally\npredictions_df &lt;- predict(best_model_object, new_data = test_data)\n# Renames default .pred column if needed by make_predictions\nPost-process Predictions: Task-specific adjustments are made. For Occupancy predictions, values are rounded to the nearest integer and floored at a minimum of 1 (since occupancy cannot be less than 1).\n# Conceptual logic from predict_on_test.R\nif (TARGET_PREDICTION == \"Occupancy\") {\n  predictions_df &lt;- predictions_df %&gt;%\n    mutate(\n      .pred = round(.pred),\n      .pred = pmax(1, .pred) # Ensure minimum of 1\n    )\n}\nSave Predictions: The resulting predictions are saved to a CSV file in the data/predictions/ directory, named convention includes the target variable, model type, and timestamp.\n# Conceptual logic from predict_on_test.R\noutput_df &lt;- predictions_df %&gt;%\n  mutate(.pred = round(.pred, digits = 4))\n# ... logic to generate output_path ...\nreadr::write_csv(output_df, output_path)\n\nThis R-based pipeline ensures that the final model is trained optimally on all available labeled data and that predictions are generated consistently using the same preprocessing steps learned during training.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Predictions</span>"
    ]
  },
  {
    "objectID": "notebooks/report/kmeans.html",
    "href": "notebooks/report/kmeans.html",
    "title": "Kmeans++",
    "section": "",
    "text": "Part B: Creating a K-Means++ Algorithm\nThough it depends on the choice of seed, running k-means with the seed used here (7560) and using only one starting set of random clusters (nstart = 1) yields a within-cluster sum of squares of 22,824.15. The algorithm took 5 iterations to converge. This metric of cluster compactness might have been lowered further if nstart was increased.\nK-means++ actually performs slightly worse, with a within-cluster sum of squares of 22,942.74 and a total of 8 iterations needed for the k-means algorithm to converge after the starting cluster centers were obtained. Visual inspection of the clusters does not suggest that k-means++ failed, rather it just incidentally produced slightly less compact clusters. Again, the k-means++ algorithm is subject to randomness and a different choice of seed might have yielded better results.\nBecause the data set is large and consists of points that lie within a relatively small subset of \\(\\mathbb{R}^2\\), the starting point selection for k-means++, which is based on distance-proportional probability, is close to a random selection of points, because each point’s probability of being selected as the next cluster center is almost 0 regardless of the point’s distance from the existing cluster centers (for this data set, the average probability of a point being selected as the next cluster is close to 1/nrow(km) \\(\\approx\\) 0.00018).\nThe need for additional iterations in the k-means++ algorithm confirms that the distance-proportional probability selection of initial cluster centers was not especially helpful for clustering this data set; there is no gain in converge speed because the initial cluster centers with the seed used (7560) are not especially good, and the algorithm needs a few more iterations to converge.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Kmeans++</span>"
    ]
  },
  {
    "objectID": "notebooks/report/kmeans.html#part-b-creating-a-k-means-algorithm",
    "href": "notebooks/report/kmeans.html#part-b-creating-a-k-means-algorithm",
    "title": "Kmeans++",
    "section": "",
    "text": "set.seed(7560)\nkm &lt;- read.csv(\"kmeans_dataset.csv\")\nkclust &lt;- kmeans(km, centers = 11, nstart = 1)\n\nlibrary(colorspace)\ncolorset &lt;- hcl(h = seq(0, 300, length.out = 11), c = 150, l = 50) \nplot(km, col = colorset[kclust$cluster],\n     main = \"K-means Clustering Assignment, K = 11 Clusters\")\npoints(kclust$centers, col = \"blue4\", pch = 18, cex = 1.5)\n\n\n\nKmeans\n\n\nsum(kclust$withinss)\n## [1] 22824.15\nkclust$iter\n## [1] 5\n\nfind_kmeanspp_centers &lt;- function(X, K){\n\n  starting_center &lt;- as.matrix(X)[sample(nrow(X), 1),]\n  \n  if (K == 1) {\n    return(matrix(starting_center, nrow = 1))\n    stop\n  }\n  \n  centers &lt;- matrix(NA, nrow = K, ncol = ncol(X), byrow = TRUE)\n  centers[1,] &lt;- starting_center\n  \n  dX &lt;- matrix(rep(starting_center, nrow(X)), nrow = nrow(X), byrow = TRUE)\n  dX &lt;- rowSums((X - dX)^2)\n  probs &lt;- dX/sum(dX)\n  \n  K_ind &lt;- 1\n  while (K &gt; K_ind) {\n    K_ind &lt;- K_ind + 1\n    \n    centers[K_ind,] &lt;- as.matrix(X)[sample(nrow(X), 1, prob = probs),]\n    \n    dC &lt;- sapply(1:K_ind, function(i) {\n      dclust &lt;- matrix(centers[i,], nrow = nrow(X), ncol = ncol(X), byrow = TRUE)\n      rowSums((X - dclust)^2)\n    })\n    \n    mindist &lt;- dC[cbind(1:nrow(dC), max.col(-dC, ties.method = \"random\"))]\n    probs &lt;- mindist/sum(mindist)\n  }\n  \n  return(centers)\n}\nset.seed(7560)\nppcenters &lt;- find_kmeanspp_centers(km, K = 11)\n\n#Sanity check: Are 'pluspluscenters' actual observations?\ncenter_indices &lt;- apply(ppcenters, 1, function(row) {\n  which(apply(km, 1, function(x) all(x == row)))\n})\nisTRUE(all(km[center_indices,] == ppcenters))\n## [1] TRUE\nkclustpp &lt;- kmeans(km, centers = ppcenters, nstart = 1)\nplot(km, col = colorset[kclustpp$cluster],\n     main = \"K-means++ Clustering Assignment, K = 11 Clusters\")\npoints(kclustpp$centers, col = \"blue4\", pch = 18, cex = 1.5)\n\n\n\nKmeans++\n\n\nsum(kclustpp$withinss)\n## [1] 22942.74\nkclustpp$iter\n## [1] 8\n\n\n\nplot(km, col = \"darkgray\", main = \"Initial Cluster Centers for K-means++\")\npoints(ppcenters, col = \"blue4\", pch = 18, cex = 1.5)\n\n\n\nKmeans Cloud",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Kmeans++</span>"
    ]
  }
]