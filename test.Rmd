---
title: "Untitled"
author: "Jason Turk"
date: "2025-04-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r, echo = FALSE, message = FALSE}
# --- Load Libraries ---
# Using suppressPackageStartupMessages to minimize console noise
suppressPackageStartupMessages(library(here)) # For locating files relative to project root
suppressPackageStartupMessages(library(dplyr)) # For data manipulation
suppressPackageStartupMessages(library(rsample)) # For data splitting and CV folds
suppressPackageStartupMessages(library(tune)) # For hyperparameter tuning
suppressPackageStartupMessages(library(yardstick)) # For model evaluation metrics
suppressPackageStartupMessages(library(workflows)) # For combining recipes and models
suppressPackageStartupMessages(library(parsnip)) # For model specifications
suppressPackageStartupMessages(library(recipes)) # For preprocessing
suppressPackageStartupMessages(library(readr)) # For reading/writing data
suppressPackageStartupMessages(library(glue)) # For formatted string output

# --- Source Helper Functions ---
# Construct full paths using here() for robustness
source(here::here("src/r/utils/data_utils.R"))
source(here::here("src/r/recipes/recipes.R"))
source(here::here("src/r/models/models.R"))
source(here::here("src/r/workflows/workflows.R"))
source(here::here("src/r/tuning/tuning.R"))
source(here::here("src/r/training/training.R"))
source(here::here("src/r/evaluation/evaluation.R"))

# --- Configuration ---
# TODO: Move these to a config file/package (e.g., config::get())
DATA_FILENAME <- "train_engineered.csv" # Use only the training data
TEST_SPLIT_PROP <- 0.8 # Proportion of data for training set
TARGET_VARIABLE <- "Duration_In_Min" # Or "Occupancy"
CV_FOLDS <- 5 # Number of cross-validation folds
SEED <- 3 # For reproducibility
TUNING_METRIC <- "rmse" # Metric to select best hyperparameters (use "accuracy" or "roc_auc" for classification)
# Features to drop (matching Python preprocess.py, excluding targets and date columns handled in recipe)
FEATURES_TO_DROP <- c(
    "Student_IDs", "Semester", "Class_Standing", "Major", "Expected_Graduation",
    "Course_Name", "Course_Number", "Course_Type", "Course_Code_by_Thousands",
    "Check_Out_Time", "Session_Length_Category"
) # Add others as needed

# Define metric set based on task (regression in this case)
# TODO: Handle classification metrics if TARGET_VARIABLE changes
MODEL_METRICS <- metric_set(rmse, rsq, mae)


# --- Pipeline Execution ---

set.seed(SEED)

# 1. Load Data
full_data <- load_data(DATA_FILENAME)

# 2. Split Data into Training and Testing
data_split <- rsample::initial_split(full_data, prop = TEST_SPLIT_PROP, strata = NULL) # Add strata if needed
train_data <- rsample::training(data_split)
test_data <- rsample::testing(data_split)

# 3. Create Recipe
recipe_obj <- create_recipe(full_data, TARGET_VARIABLE, FEATURES_TO_DROP)
#print(recipe_obj) # Print summary of recipe steps

prepped_recipe <- prep(recipe_obj, training = full_data)
maintrain <- bake(prepped_recipe, new_data = full_data)
```


**The `dropout layer` randomly sets a fraction of the input units to 0 at each update during training, which helps prevent each neuron from becoming too reliant on any one input. 
The `weight decay` parameter is used to apply L2 regularization to the weights of the model, which helps prevent overfiting by penalizing large weights.**