[
    {
        "id": "LESSON_TUNING_NAN_METRICS_001",
        "type": "Lesson",
        "name": "NaN Metrics in tune::tune_grid with Small Data/Folds",
        "problem_pattern": {
            "description": "tune::tune_grid produces NaN values for metrics like R-squared (rsq) when cross-validation folds contain assessment sets with zero variance in the outcome variable.",
            "error_messages": [
                "NaN produced",
                "Error in `.filter_perf_metrics(x, metric, eval_time)`: No results are available. Please use `collect_metrics()`... (when calling select_best)"
            ],
            "context": "Running testthat tests for tidymodels tuning functions (`tune_model_grid`, `select_best_hyperparameters`) using very small sample data (e.g., <10 rows) and low number of CV folds (e.g., v=2)."
        },
        "solution": {
            "steps": [
                "Increase the size of the sample data used for testing, ensuring sufficient data points and outcome variance within each cross-validation fold.",
                "Example: Replicate the original small sample data multiple times using `dplyr::bind_rows(replicate(n, small_data, simplify = FALSE))`.",
                "Optionally add minor random noise to the outcome variable in the test data to further ensure variance: `mutate(outcome = outcome + rnorm(n(), 0, small_sd))`."
            ],
            "verification": "Re-running the testthat tests results in valid numeric values for all calculated metrics, and `tune::select_best()` works correctly for all metrics."
        },
        "environment": {
            "packages": [
                "tune",
                "rsample",
                "yardstick",
                "testthat",
                "dplyr"
            ],
            "r_version": "(not specified, likely general)",
            "os": "(not specified, likely general)"
        },
        "metadata": {
            "creation_date": "2024-07-26T14:00:00Z",
            "last_updated_date": "2024-07-26T14:00:00Z",
            "tags": [
                "tidymodels",
                "tune",
                "testthat",
                "cross-validation",
                "NaN",
                "rsq"
            ]
        }
    },
    {
        "id": "LESSON_TESTING_FINALIZED_PARAMS_001",
        "type": "Lesson",
        "name": "Verifying Finalized Hyperparameters in Fitted Workflows Tests",
        "problem_pattern": {
            "description": "Tests checking hyperparameter values directly within the `$args` of a model specification extracted (`extract_spec_parsnip`) from a *fitted* workflow object (`parsnip::fit`) may fail because the stored spec args might not reflect the values used during the fit.",
            "error_messages": [
                "finalized_spec$args$param not equal to best_params$param",
                "parsnip_fit$spec$args$param not equal to best_params$param"
            ],
            "context": "Writing testthat tests for a function that finalizes and fits a tidymodels workflow (`finalize_workflow()` then `fit()`). Attempting to verify that the hyperparameters were correctly passed by inspecting the spec object within the returned fitted workflow."
        },
        "solution": {
            "steps": [
                "Avoid directly testing the equality of `$args` values in the spec extracted from the *fitted* workflow.",
                "Instead, test the finalization step separately: Call `finalize_workflow()` explicitly and check the `$args` in the spec of the *returned finalized-but-not-fitted* workflow.",
                "For the function that performs the fit (`train_final_model`):",
                "  - Test that the function returns a workflow object (`is_workflow()`).",
                "  - Test that the returned workflow is marked as trained (`fitted_model$trained == TRUE`).",
                "  - Test that the trained recipe can be extracted without error (`expect_no_error(extract_recipe(fitted_model, estimated = TRUE))`).",
                "  - (Recommended Addition): Test that the fitted workflow can make predictions without error (`expect_no_error(predict(fitted_model, new_data = ...))`).",
                "  - (Optional/Engine-Specific): If necessary, check parameters in the underlying fitted engine object (`extract_fit_engine()`), acknowledging this is less general."
            ],
            "verification": "Tests focusing on the state *before* fitting (for finalization) and the usability/state *after* fitting (for the fitting function) pass reliably."
        },
        "environment": {
            "packages": [
                "workflows",
                "tune",
                "parsnip",
                "testthat",
                "rlang"
            ],
            "r_version": "(not specified, likely general)",
            "os": "(not specified, likely general)"
        },
        "metadata": {
            "creation_date": "2024-07-26T14:05:00Z",
            "last_updated_date": "2024-07-26T14:05:00Z",
            "tags": [
                "tidymodels",
                "workflows",
                "finalize_workflow",
                "fit",
                "testthat",
                "hyperparameters"
            ]
        }
    },
    {
        "id": "LESSON_PYTEST_IMPORT_RESOLUTION_001",
        "type": "Lesson",
        "name": "Resolving Internal Imports within Modules Under Test in Pytest",
        "problem_pattern": {
            "description": "Pytest fails to collect tests due to ModuleNotFoundError/ImportError when the test script imports a module (e.g., module A) which itself imports another local module (e.g., module B using 'from .B import ...' or 'from B import ...'). This occurs even if the directory containing both modules A and B is added to sys.path in the test script.",
            "error_messages": [
                "ModuleNotFoundError: No module named 'B'",
                "ImportError: attempted relative import with no known parent package",
                "ERROR collecting tests/python/test_module_a.py",
                "Interrupted: 1 error during collection"
            ],
            "context": "Running pytest from the project root. Test script in tests/ modifies sys.path to include src/python/models. Test script imports module src/python/models/module_a.py. Module A tries to import src/python/models/module_b.py."
        },
        "solution": {
            "steps": [
                "Ensure necessary __init__.py files exist in the directories to make them packages (e.g., src/python/ and src/python/models/).",
                "In the test script (e.g., tests/python/test_module_a.py), modify sys.path to include the *parent directory* containing the modules (e.g., add src/python/models to sys.path).",
                "Within the module being imported by the first module (e.g., in module_a.py), change the internal import from relative (from .module_b import ...) to absolute (from module_b import ...). This relies on the parent directory (src/python/models) being present in sys.path when the test runs.",
                "Verify the fix by running pytest again; tests should now be collected and executed."
            ],
            "verification": "Pytest successfully collects and runs tests without ModuleNotFoundError related to the internal import."
        },
        "environment": {
            "packages": [
                "pytest",
                "torch"
            ],
            "python_version": "3.11",
            "os": "linux"
        },
        "metadata": {
            "creation_date": "2024-07-26T16:05:00Z",
            "last_updated_date": "2024-07-26T16:05:00Z",
            "tags": [
                "pytest",
                "import",
                "ModuleNotFoundError",
                "ImportError",
                "relative import",
                "absolute import",
                "sys.path",
                "test collection"
            ]
        }
    },
    {
        "id": "LESSON_PYTEST_COMMAND_NOT_FOUND_001",
        "type": "Lesson",
        "name": "Resolving 'pytest: command not found' in User Site-Packages Installations",
        "problem_pattern": {
            "description": "Running the `pytest` command directly in the shell results in 'bash: pytest: command not found', even though `pip show pytest` or `pip install pytest` confirms it is installed.",
            "error_messages": [
                "bash: pytest: command not found"
            ],
            "context": "Working within a Python environment (e.g., virtual environment, dev container) where packages are installed into the user's local site-packages directory (e.g., ~/.local/lib/pythonX.Y/site-packages). The corresponding script directory (e.g., ~/.local/bin) is not included in the shell's PATH environment variable."
        },
        "solution": {
            "steps": [
                "Verify pytest installation location using `pip show pytest`.",
                "Confirm that the user's local bin directory (e.g., `~/.local/bin`) is not in the output of `echo $PATH`.",
                "**Workaround:** Execute pytest as a Python module using `python -m pytest [arguments]`. This bypasses the need for the executable to be in the PATH.",
                "**Alternative (Permanent Fix):** Add the user's local bin directory to the PATH. Add `export PATH=\"$PATH:$HOME/.local/bin\"` to the shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`) and reload the shell or open a new terminal."
            ],
            "verification": "Running `python -m pytest [arguments]` successfully executes the tests. Alternatively, after modifying PATH, running `pytest [arguments]` directly works."
        },
        "environment": {
            "packages": [
                "pytest",
                "pip"
            ],
            "python_version": "Any",
            "os": "linux/unix-like"
        },
        "metadata": {
            "creation_date": "2024-07-26T17:15:00Z",
            "last_updated_date": "2024-07-26T17:15:00Z",
            "tags": [
                "pytest",
                "command not found",
                "PATH",
                "python -m",
                "pip",
                "user site-packages",
                "environment setup"
            ]
        }
    },
    {
        "id": "LESSON_PYTORCH_TENSOR_TYPEERROR_BOOL_OBJECT_001",
        "type": "Lesson",
        "name": "Resolving PyTorch TypeError for DataFrame with Boolean Columns",
        "problem_pattern": {
            "description": "Calling `torch.tensor(dataframe.values)` fails with `TypeError: can't convert np.ndarray of type numpy.object_.` even if the DataFrame appears numeric, often because it contains boolean columns.",
            "error_messages": [
                "TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
            ],
            "context": "Converting a pandas DataFrame (potentially resulting from preprocessing steps like pd.get_dummies) to a PyTorch tensor for use in a Dataset or model. The DataFrame contains columns with dtype 'bool' alongside other numeric types (int, float)."
        },
        "solution": {
            "steps": [
                "Before converting the DataFrame to a tensor, explicitly convert all boolean columns to a numeric type that PyTorch supports (e.g., int or float).",
                "Example:",
                "  `for col in df.columns:`",
                "  `    if df[col].dtype == 'bool':`",
                "  `        df[col] = df[col].astype(int)`",
                "Then proceed with `torch.tensor(df.values)`."
            ],
            "verification": "The `torch.tensor()` conversion succeeds without raising a TypeError."
        },
        "environment": {
            "packages": [
                "torch",
                "pandas",
                "numpy"
            ],
            "python_version": "Any",
            "os": "Any"
        },
        "metadata": {
            "creation_date": "2024-07-26T18:20:00Z",
            "last_updated_date": "2024-07-26T18:20:00Z",
            "tags": [
                "pytorch",
                "pandas",
                "tensor",
                "TypeError",
                "boolean",
                "dtype",
                "object",
                "conversion"
            ]
        }
    },
    {
        "id": "LESSON_OPTUNA_NAMEERROR_OPTIM_001",
        "type": "Lesson",
        "name": "Resolving NameError for optim inside Optuna Objective",
        "problem_pattern": {
            "description": "When defining an optimizer inside an Optuna objective function using `optim.Adam(...)` after importing `torch.optim as optim` at the top level, a `NameError: name 'optim' is not defined` occurs during study execution.",
            "error_messages": [
                "NameError: name 'optim' is not defined",
                "Trial X failed because of the following error: NameError..."
            ],
            "context": "Using Optuna to tune hyperparameters for a PyTorch model. The optimizer is instantiated within the objective function passed to `study.optimize()`."
        },
        "solution": {
            "steps": [
                "Explicitly reference the full module path when creating the optimizer inside the objective function.",
                "Change `optimizer = optim.Adam(...)` to `optimizer = torch.optim.Adam(...)`."
            ],
            "verification": "The Optuna study runs without raising the NameError related to the optimizer."
        },
        "environment": {
            "packages": [
                "optuna",
                "torch"
            ],
            "python_version": "3.11",
            "os": "linux"
        },
        "metadata": {
            "creation_date": "2025-04-13T04:45:00Z",
            "last_updated_date": "2025-04-13T04:45:00Z",
            "tags": [
                "optuna",
                "pytorch",
                "hyperparameter tuning",
                "NameError",
                "scope",
                "objective function"
            ]
        }
    },
    {
        "id": "LESSON_PYTHON_CLASS_SCOPE_IMPORT_001",
        "type": "Lesson",
        "name": "Resolving NameError for Class Used in Multiple Methods",
        "problem_pattern": {
            "description": "A `NameError: name 'ClassName' is not defined` occurs within a class method (e.g., `forward`) when trying to use a class (`ClassName`) that was imported *only* within another method (e.g., `__init__`) of the same class.",
            "error_messages": [
                "NameError: name 'ClassName' is not defined"
            ],
            "context": "Using `isinstance(obj, ClassName)` or calling `ClassName()` within a `forward` method, where `ClassName` was imported only within the `__init__` method of the same PyTorch Module."
        },
        "solution": {
            "steps": [
                "Import the required class (`ClassName`) at the top level of the module file (e.g., outside the class definition).",
                "Remove the redundant import statement from the specific method (e.g., `__init__`).",
                "This makes the imported name available throughout the module scope, including all methods of the class defined within that module."
            ],
            "verification": "The code runs without the `NameError` when accessing the imported class name in different methods."
        },
        "environment": {
            "packages": [
                "Any"
            ],
            "python_version": "Any",
            "os": "Any"
        },
        "metadata": {
            "creation_date": "2025-04-23T12:25:00Z",
            "last_updated_date": "2025-04-23T12:25:00Z",
            "tags": [
                "python",
                "import",
                "scope",
                "NameError",
                "class",
                "module",
                "pytorch"
            ]
        }
    },
    {
        "id": "LESSON_PYTHON_TRANSITIVE_IMPORT_ERROR_001",
        "type": "Lesson",
        "name": "Resolving ModuleNotFoundError Due to Dependency within Imported Module",
        "problem_pattern": {
            "description": "A `ModuleNotFoundError: No module named 'DependencyModule'` occurs when running a script that imports ModuleA, where ModuleA itself contains an import statement for `DependencyModule` (e.g., `from DependencyModule import ...`). The error indicates Python cannot find `DependencyModule` when ModuleA tries to import it.",
            "error_messages": [
                "ModuleNotFoundError: No module named 'DependencyModule'"
            ],
            "context": "Running a main script (e.g., `training.py`) that imports `ModelArchitecture` from `models/architecture.py`. The `architecture.py` file imports `ComponentModel` from `models/component.py`. The `component.py` file imports helpers from `utils/helpers.py` (or previously from a missing `mingru.py`). The error occurs when `component.py` tries to import its dependency."
        },
        "solution": {
            "steps": [
                "Ensure the dependency module (`DependencyModule` or `helpers.py`) exists at the expected location.",
                "Verify that the import statement within the intermediate module (ModuleA / `component.py`) correctly points to the dependency module, considering the project structure and Python's import resolution rules (absolute vs. relative imports).",
                "Ensure the main script (`training.py`) correctly modifies `sys.path` *before* importing modules, adding the necessary root directories (e.g., the `src` directory) so that absolute imports like `from python.utils.helpers import ...` can be resolved when the intermediate module is loaded.",
                "If the dependency was previously imported from an incorrect/missing file (e.g., `mingru.py`), refactor the intermediate module to import from the correct location (e.g., `rnn_helpers.py`) and ensure the helper file exists and is accessible via `sys.path`."
            ],
            "verification": "The main script runs without the `ModuleNotFoundError` related to the transitive dependency."
        },
        "environment": {
            "packages": [
                "Any"
            ],
            "python_version": "Any",
            "os": "Any"
        },
        "metadata": {
            "creation_date": "2025-04-23T12:20:00Z",
            "last_updated_date": "2025-04-23T12:25:00Z",
            "tags": [
                "python",
                "import",
                "ModuleNotFoundError",
                "dependency",
                "sys.path",
                "package structure"
            ]
        }
    },
    {
        "id": "LESSON_RNN_TABULAR_MODELING_APPROACH_001",
        "type": "Lesson",
        "name": "Choosing RNN Modeling Approach for Tabular Data",
        "problem_pattern": {
            "description": "Deciding how to structure input for an RNN (GRU/LSTM) when working with tabular data.",
            "context": "Applying RNN models to a dataset where each row represents an observation, potentially with a temporal ordering."
        },
        "solution": {
            "steps": [
                "**Approach 1: Independent Row Processing (Sequence Length = 1)**",
                "   - Treat each row independently.",
                "   - Reshape input in the model's `forward` method: `x = x.unsqueeze(1)` -> `(batch, 1, features)`.",
                "   - Use case: When there's no meaningful sequence or temporal relationship between rows, or when using the RNN primarily as a complex feature transformer.",
                "   - Implications: RNN doesn't learn inter-row dependencies. `sequence_length` is not a tunable parameter.",
                "   - Dataset: Use a standard `TabularDataset`.",
                "**Approach 2: Sequential Row Processing (Sequence Length > 1)**",
                "   - Treat rows as steps in a sequence (requires data to be meaningfully sorted, e.g., chronologically).",
                "   - Create a `SequenceDataset` that yields input sequences of shape `(sequence_length, num_features)` and the target corresponding to the last row.",
                "   - Do *not* `unsqueeze` input in the `forward` method; expect `(batch, sequence_length, features)`.",
                "   - Take the RNN output from the last time step (`output[:, -1, :]`) for prediction.",
                "   - Use case: When there are temporal dependencies between rows that the model should learn.",
                "   - Implications: RNN learns sequential patterns. `sequence_length` becomes a tunable hyperparameter."
            ],
            "verification": "The chosen approach aligns with the data characteristics (time-sorted or independent rows) and the modeling goal (feature transformation vs. sequence modeling). Training proceeds correctly with the corresponding Dataset and model forward pass logic."
        },
        "environment": {
            "packages": [
                "torch",
                "pytorch"
            ],
            "python_version": "Any",
            "os": "Any"
        },
        "metadata": {
            "creation_date": "2025-04-23T12:35:00Z",
            "last_updated_date": "2025-04-23T12:35:00Z",
            "tags": [
                "pytorch",
                "rnn",
                "lstm",
                "gru",
                "tabular data",
                "sequence modeling",
                "data preparation",
                "Dataset",
                "modeling strategy"
            ]
        }
    },
    {
        "id": "LESSON_PYTORCH_NAN_IN_CUSTOM_RNN_SCAN_001",
        "type": "Lesson",
        "name": "NaNs Caused by Unstable Custom Parallel Scan or Forward Pass",
        "problem_pattern": {
            "description": "Model forward pass produces NaN values immediately, especially with extreme inputs, OR training loop produces NaNs after several epochs. Issue often related to custom RNN layers (e.g., minLSTM) using shared helper functions for parallel processing (e.g., heinsen_associative_scan_log).",
            "error_messages": [
                "AssertionError: Output tensor contains NaNs",
                "AssertionError: train_step returned NaN loss",
                "AssertionError: evaluate_model returned NaN RMSE"
            ],
            "context": "Implementing a custom RNN (like minLSTM) using sequence lengths > 1, relying on a shared helper function (rnn_helpers.py). The helper function's scan implementation or the RNN layer's handling of large intermediate values might be unstable."
        },
        "solution": {
            "steps": [
                "Isolate the issue using TDD: Test data pipeline (preprocessing, scaling, Dataset) first.",
                "Test the model's forward pass with clean, simple input (e.g., torch.randn). If NaNs appear -> Suspect model component.",
                "Test the model's forward pass with extreme (but finite) inputs (e.g., +/- 1e8). If NaNs appear -> Suspect numerical stability within model/components.",
                "Investigate custom components like parallel scans (e.g., heinsen_associative_scan_log in rnn_helpers.py). Check for correctness, stability (edge cases, large values), and compatibility.",
                "Replace faulty/placeholder scan implementations. (e.g., Use logcumsumexp-based scan from mingru.py).",
                "If forward pass still produces NaNs with extreme inputs, add clamping/nan_to_num *inside* the problematic component (e.g., clamp the output of log_h.exp() in the scan function).",
                "If forward pass is stable but NaNs appear later in training -> Suspect backward pass instability (exploding gradients)."
            ],
            "verification": "Pytest tests for forward pass (including with extreme inputs) pass. Subsequent NaNs during training likely require gradient clipping."
        },
        "environment": {
            "packages": [
                "pytorch",
                "pytest",
                "numpy"
            ],
            "python_version": "3.11",
            "os": "linux"
        },
        "metadata": {
            "creation_date": "PLACEHOLDER_TIMESTAMP",
            "last_updated_date": "PLACEHOLDER_TIMESTAMP_LATEST",
            "tags": [
                "pytorch",
                "rnn",
                "lstm",
                "NaN",
                "numerical stability",
                "parallel scan",
                "logcumsumexp",
                "debugging",
                "TDD",
                "forward pass",
                "clamping"
            ]
        }
    },
    {
        "id": "LESSON_PYTORCH_NAN_GRADIENTS_BACKWARD_PASS_001",
        "type": "Lesson",
        "name": "NaN Gradients Produced During Backward Pass",
        "problem_pattern": {
            "description": "Model forward pass succeeds (even with extreme inputs), but NaNs appear in parameter gradients (`.grad`) immediately after calling `loss.backward()`.",
            "error_messages": [
                "AssertionError: NaNs found in gradients after backward pass"
            ],
            "context": "Training a model (potentially with custom layers or complex interactions) where intermediate activations during the forward pass become very large, leading to numerical overflow or invalid operations during the gradient calculation in the backward pass."
        },
        "solution": {
            "steps": [
                "Verify forward pass stability first using TDD (including checks with extreme inputs).",
                "Add a dedicated test to check parameter gradients for NaNs immediately after `loss.backward()`:",
                "  - Perform forward pass and loss calculation manually in the test.",
                "  - Call `loss.backward()`.",
                "  - Iterate through `model.parameters()` and check if `p.grad` contains NaNs using `torch.isnan(p.grad).any()`.",
                "If NaNs are found in gradients, the primary solution is **Gradient Clipping**.",
                "Implement gradient clipping (e.g., `torch.nn.utils.clip_grad_norm_`) in the training step *after* `loss.backward()` and *before* `optimizer.step()`.",
                "Start with a conventional clipping value (e.g., 1.0 or 0.5) and adjust if instability persists.",
                "Other potential (less common) causes/solutions: Lowering learning rate, using mixed-precision training carefully, checking for divisions by zero in custom backward functions (if any)."
            ],
            "verification": "The dedicated gradient stability test may still fail (as it checks *before* clipping), but the main `train_step` test (which includes clipping) should pass, and full training runs should become more stable."
        },
        "environment": {
            "packages": [
                "pytorch",
                "pytest",
                "numpy"
            ],
            "python_version": "3.11",
            "os": "linux"
        },
        "metadata": {
            "creation_date": "PLACEHOLDER_TIMESTAMP_LATEST",
            "last_updated_date": "PLACEHOLDER_TIMESTAMP_LATEST",
            "tags": [
                "pytorch",
                "NaN",
                "gradients",
                "backward pass",
                "exploding gradients",
                "gradient clipping",
                "debugging",
                "TDD"
            ]
        }
    }
]